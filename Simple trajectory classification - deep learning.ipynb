{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and Result \n",
    "\n",
    "Using an appropriately tuned deep feed-forward neural network to train the data ($2000$ training examples, with each example consisting of a time series/trajectory for the $x$-coordinate of 102 steps), we found that accuracy on the test data ($1000$ test examples, with each example consisting of a trajectory for the $x$-coordinate of 102 steps) is around $73 \\%$, which could be improved by using, for instance, more training data, including the trajectory for the $y$-coordinate in the training data set, or a more sophisticated neural network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# First, we get the data\n",
    "dataset0 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m0.csv\",header=None)\n",
    "dataset1 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m1.csv\",header=None)\n",
    "dataset2 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m2.csv\",header=None)\n",
    "\n",
    "li = []\n",
    "li.append(dataset0)\n",
    "li.append(dataset1)\n",
    "li.append(dataset2)\n",
    "\n",
    "dataset = np.array(pd.concat(li, axis=0, ignore_index=True))\n",
    "np.random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "np.savetxt('all_traj.csv', np.c_[dataset], delimiter=',') \n",
    "\n",
    "dataset.head(10)\n",
    "row=np.shape(dataset)[0]\n",
    "col=np.shape(dataset)[1]\n",
    "print(row); print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 102)\n",
      "(1000, 102)\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "num_train = int(row*2/3)\n",
    "training_set = dataset[:num_train].iloc[:,0:col].values\n",
    "test_set = dataset[num_train:].iloc[:,0:col].values\n",
    "print(np.shape(training_set)); print(np.shape(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>0.008009</td>\n",
       "      <td>-0.010139</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>-0.029256</td>\n",
       "      <td>-0.042169</td>\n",
       "      <td>-0.068667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052008</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>-0.033684</td>\n",
       "      <td>-0.009525</td>\n",
       "      <td>-0.027281</td>\n",
       "      <td>-0.030483</td>\n",
       "      <td>-0.041839</td>\n",
       "      <td>-0.054744</td>\n",
       "      <td>-0.067227</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013381</td>\n",
       "      <td>0.017872</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>-0.007943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458650</td>\n",
       "      <td>-0.490972</td>\n",
       "      <td>-0.512801</td>\n",
       "      <td>-0.512246</td>\n",
       "      <td>-0.496050</td>\n",
       "      <td>-0.502954</td>\n",
       "      <td>-0.501706</td>\n",
       "      <td>-0.487807</td>\n",
       "      <td>-0.477138</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.037596</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.025684</td>\n",
       "      <td>0.043444</td>\n",
       "      <td>0.051707</td>\n",
       "      <td>0.053185</td>\n",
       "      <td>0.048018</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.024177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092414</td>\n",
       "      <td>0.091177</td>\n",
       "      <td>0.098155</td>\n",
       "      <td>0.084328</td>\n",
       "      <td>0.077293</td>\n",
       "      <td>0.066811</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>0.073460</td>\n",
       "      <td>0.120050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>0.007790</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>-0.007581</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.024470</td>\n",
       "      <td>0.012727</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146325</td>\n",
       "      <td>0.169303</td>\n",
       "      <td>0.194519</td>\n",
       "      <td>0.188065</td>\n",
       "      <td>0.183606</td>\n",
       "      <td>0.189446</td>\n",
       "      <td>0.197405</td>\n",
       "      <td>0.190020</td>\n",
       "      <td>0.205637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.010173</td>\n",
       "      <td>0.011853</td>\n",
       "      <td>-0.001674</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.007944</td>\n",
       "      <td>-0.026456</td>\n",
       "      <td>-0.034731</td>\n",
       "      <td>-0.056660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539661</td>\n",
       "      <td>-0.547715</td>\n",
       "      <td>-0.534951</td>\n",
       "      <td>-0.519082</td>\n",
       "      <td>-0.527979</td>\n",
       "      <td>-0.564180</td>\n",
       "      <td>-0.557321</td>\n",
       "      <td>-0.560317</td>\n",
       "      <td>-0.576512</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.023761</td>\n",
       "      <td>0.029217</td>\n",
       "      <td>0.046976</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>-0.005505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027107</td>\n",
       "      <td>-0.011182</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.026486</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005443</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>-0.020492</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>-0.021155</td>\n",
       "      <td>-0.021481</td>\n",
       "      <td>-0.002577</td>\n",
       "      <td>-0.005792</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352719</td>\n",
       "      <td>-0.368043</td>\n",
       "      <td>-0.365271</td>\n",
       "      <td>-0.366456</td>\n",
       "      <td>-0.378613</td>\n",
       "      <td>-0.395740</td>\n",
       "      <td>-0.395808</td>\n",
       "      <td>-0.406339</td>\n",
       "      <td>-0.388433</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002420</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.020471</td>\n",
       "      <td>-0.011108</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>-0.012705</td>\n",
       "      <td>-0.009855</td>\n",
       "      <td>-0.001137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018343</td>\n",
       "      <td>-0.030837</td>\n",
       "      <td>-0.056446</td>\n",
       "      <td>-0.041564</td>\n",
       "      <td>-0.009953</td>\n",
       "      <td>-0.008850</td>\n",
       "      <td>-0.008732</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027301</td>\n",
       "      <td>-0.011466</td>\n",
       "      <td>-0.030466</td>\n",
       "      <td>-0.062386</td>\n",
       "      <td>-0.061362</td>\n",
       "      <td>-0.067246</td>\n",
       "      <td>-0.094593</td>\n",
       "      <td>-0.107024</td>\n",
       "      <td>-0.120600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391666</td>\n",
       "      <td>-0.427885</td>\n",
       "      <td>-0.447803</td>\n",
       "      <td>-0.444593</td>\n",
       "      <td>-0.449535</td>\n",
       "      <td>-0.423673</td>\n",
       "      <td>-0.428646</td>\n",
       "      <td>-0.417457</td>\n",
       "      <td>-0.426954</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>-0.008136</td>\n",
       "      <td>-0.005089</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>-0.009116</td>\n",
       "      <td>-0.056756</td>\n",
       "      <td>-0.074604</td>\n",
       "      <td>-0.064575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589496</td>\n",
       "      <td>-0.620288</td>\n",
       "      <td>-0.621947</td>\n",
       "      <td>-0.637385</td>\n",
       "      <td>-0.641903</td>\n",
       "      <td>-0.627130</td>\n",
       "      <td>-0.639971</td>\n",
       "      <td>-0.626084</td>\n",
       "      <td>-0.635690</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0  0.0  0.014233  0.018073  0.033469  0.008009 -0.010139  0.000483 -0.029256   \n",
       "1  0.0  0.013381  0.017872  0.011803  0.017269  0.000612  0.003238  0.008392   \n",
       "2  0.0 -0.037596  0.016685  0.025684  0.043444  0.051707  0.053185  0.048018   \n",
       "3  0.0 -0.007297  0.007790  0.002695  0.000978 -0.007581  0.009402  0.024470   \n",
       "4  0.0 -0.000030 -0.010173  0.011853 -0.001674  0.001349 -0.007944 -0.026456   \n",
       "5  0.0  0.000535  0.003537  0.023761  0.029217  0.046976  0.038772  0.032941   \n",
       "6  0.0 -0.005443  0.022262 -0.020492 -0.023009 -0.021155 -0.021481 -0.002577   \n",
       "7  0.0 -0.002420  0.010192 -0.020471 -0.011108  0.000158  0.004495 -0.012705   \n",
       "8  0.0 -0.027301 -0.011466 -0.030466 -0.062386 -0.061362 -0.067246 -0.094593   \n",
       "9  0.0  0.006047 -0.008136 -0.005089  0.018195 -0.000838 -0.009116 -0.056756   \n",
       "\n",
       "        8         9    ...       92        93        94        95        96   \\\n",
       "0 -0.042169 -0.068667  ... -0.052008 -0.036385 -0.033684 -0.009525 -0.027281   \n",
       "1  0.011141 -0.007943  ... -0.458650 -0.490972 -0.512801 -0.512246 -0.496050   \n",
       "2  0.038871  0.024177  ...  0.092414  0.091177  0.098155  0.084328  0.077293   \n",
       "3  0.012727  0.020929  ...  0.146325  0.169303  0.194519  0.188065  0.183606   \n",
       "4 -0.034731 -0.056660  ... -0.539661 -0.547715 -0.534951 -0.519082 -0.527979   \n",
       "5  0.011174 -0.005505  ... -0.027107 -0.011182  0.002101  0.007328  0.020003   \n",
       "6 -0.005792  0.007509  ... -0.352719 -0.368043 -0.365271 -0.366456 -0.378613   \n",
       "7 -0.009855 -0.001137  ... -0.018343 -0.030837 -0.056446 -0.041564 -0.009953   \n",
       "8 -0.107024 -0.120600  ... -0.391666 -0.427885 -0.447803 -0.444593 -0.449535   \n",
       "9 -0.074604 -0.064575  ... -0.589496 -0.620288 -0.621947 -0.637385 -0.641903   \n",
       "\n",
       "        97        98        99        100  101  \n",
       "0 -0.030483 -0.041839 -0.054744 -0.067227  0.0  \n",
       "1 -0.502954 -0.501706 -0.487807 -0.477138  1.0  \n",
       "2  0.066811  0.082621  0.073460  0.120050  0.0  \n",
       "3  0.189446  0.197405  0.190020  0.205637  0.0  \n",
       "4 -0.564180 -0.557321 -0.560317 -0.576512  1.0  \n",
       "5  0.034761  0.026486  0.013341 -0.005342  0.0  \n",
       "6 -0.395740 -0.395808 -0.406339 -0.388433  2.0  \n",
       "7 -0.008850 -0.008732 -0.024432 -0.026219  0.0  \n",
       "8 -0.423673 -0.428646 -0.417457 -0.426954  2.0  \n",
       "9 -0.627130 -0.639971 -0.626084 -0.635690  2.0  \n",
       "\n",
       "[10 rows x 102 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.DataFrame(training_set)\n",
    "training_df.head(10)\n",
    "\n",
    "#pd.plotting.scatter_matrix(training_df, c=training_set['750'], figsize=(15,15), marker='o', s=60)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 train samples\n",
      "1000 test samples\n",
      "(2000, 101)\n",
      "(1000, 101)\n",
      "(2000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "#from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 500\n",
    "num_classes = 3\n",
    "epochs = 100\n",
    "\n",
    "# the data, split between train and test sets\n",
    "x_train=training_set[:,:col-1]\n",
    "y_train=training_set[:,col-1:]\n",
    "x_test=test_set[:,:col-1]\n",
    "y_test=test_set[:,col-1:]\n",
    "\n",
    "x_train = x_train.reshape(num_train, col-1)\n",
    "x_test = x_test.reshape(row-num_train, col-1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(np.shape(x_train)); print(np.shape(x_test))\n",
    "print(np.shape(y_train)); print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_238 (Dense)            (None, 512)               52224     \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 316,419\n",
      "Trainable params: 316,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.9790 - acc: 0.4670 - val_loss: 0.8363 - val_acc: 0.5730\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 69us/step - loss: 0.7992 - acc: 0.5890 - val_loss: 0.7745 - val_acc: 0.6150\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.7614 - acc: 0.5910 - val_loss: 0.7415 - val_acc: 0.6430\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.7167 - acc: 0.6500 - val_loss: 0.7310 - val_acc: 0.6460\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.6994 - acc: 0.6385 - val_loss: 0.6941 - val_acc: 0.6570\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.6600 - acc: 0.6785 - val_loss: 0.6890 - val_acc: 0.6610\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.6878 - acc: 0.6455 - val_loss: 0.6309 - val_acc: 0.7120\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 0.6421 - acc: 0.6895 - val_loss: 0.6420 - val_acc: 0.6760\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.6250 - acc: 0.6920 - val_loss: 0.6322 - val_acc: 0.7030\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.6888 - acc: 0.6535 - val_loss: 0.6311 - val_acc: 0.7030\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.6194 - acc: 0.7035 - val_loss: 0.6105 - val_acc: 0.7080\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 0.6199 - acc: 0.7025 - val_loss: 0.6466 - val_acc: 0.6830\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.6113 - acc: 0.7035 - val_loss: 0.6528 - val_acc: 0.6780\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.6239 - acc: 0.6920 - val_loss: 0.6267 - val_acc: 0.7050\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 0.6197 - acc: 0.6885 - val_loss: 0.6647 - val_acc: 0.6640\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.6141 - acc: 0.7015 - val_loss: 0.6132 - val_acc: 0.7020\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.6113 - acc: 0.7095 - val_loss: 0.5969 - val_acc: 0.7140\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.6214 - acc: 0.7030 - val_loss: 0.6047 - val_acc: 0.7160\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.5954 - acc: 0.7170 - val_loss: 0.6337 - val_acc: 0.6800\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.6170 - acc: 0.6940 - val_loss: 0.5949 - val_acc: 0.7210\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.5873 - acc: 0.7280 - val_loss: 0.5922 - val_acc: 0.7330\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.6285 - acc: 0.6705 - val_loss: 0.6937 - val_acc: 0.6460\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.6286 - acc: 0.6870 - val_loss: 0.6089 - val_acc: 0.7150\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.6099 - acc: 0.7035 - val_loss: 0.6039 - val_acc: 0.7020\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.5986 - acc: 0.7020 - val_loss: 0.6216 - val_acc: 0.6830\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.5938 - acc: 0.7115 - val_loss: 0.6105 - val_acc: 0.7050\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.5829 - acc: 0.7260 - val_loss: 0.6088 - val_acc: 0.6940\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5961 - acc: 0.7175 - val_loss: 0.6155 - val_acc: 0.6890\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.6195 - acc: 0.6805 - val_loss: 0.5984 - val_acc: 0.7160\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.5860 - acc: 0.7225 - val_loss: 0.6029 - val_acc: 0.7000\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.5905 - acc: 0.7100 - val_loss: 0.6016 - val_acc: 0.7280\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.5928 - acc: 0.7155 - val_loss: 0.5995 - val_acc: 0.7040\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5879 - acc: 0.7265 - val_loss: 0.6417 - val_acc: 0.6870\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.5814 - acc: 0.7260 - val_loss: 0.6416 - val_acc: 0.6930\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.6266 - acc: 0.6950 - val_loss: 0.5882 - val_acc: 0.7220\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.5720 - acc: 0.7340 - val_loss: 0.5828 - val_acc: 0.7110\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5788 - acc: 0.7265 - val_loss: 0.6157 - val_acc: 0.6890\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.6164 - acc: 0.6875 - val_loss: 0.5966 - val_acc: 0.7220\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.5716 - acc: 0.7320 - val_loss: 0.5910 - val_acc: 0.7280\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.5707 - acc: 0.7340 - val_loss: 0.5956 - val_acc: 0.7080\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.6108 - acc: 0.7030 - val_loss: 0.5942 - val_acc: 0.7120\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.5794 - acc: 0.7200 - val_loss: 0.5889 - val_acc: 0.7180\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.5724 - acc: 0.7410 - val_loss: 0.5943 - val_acc: 0.7080\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.5790 - acc: 0.7250 - val_loss: 0.5852 - val_acc: 0.7120\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.5727 - acc: 0.7300 - val_loss: 0.7138 - val_acc: 0.6580\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.6098 - acc: 0.6880 - val_loss: 0.5840 - val_acc: 0.7230\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.5635 - acc: 0.7395 - val_loss: 0.5924 - val_acc: 0.7330\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.5561 - acc: 0.7365 - val_loss: 0.6416 - val_acc: 0.6920\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.6066 - acc: 0.6970 - val_loss: 0.5863 - val_acc: 0.7270\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.5687 - acc: 0.7365 - val_loss: 0.6000 - val_acc: 0.7060\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.5541 - acc: 0.7340 - val_loss: 0.5851 - val_acc: 0.7290\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.5647 - acc: 0.7340 - val_loss: 0.5793 - val_acc: 0.7280\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.5467 - acc: 0.7445 - val_loss: 0.6228 - val_acc: 0.7100\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.5902 - acc: 0.7135 - val_loss: 0.5805 - val_acc: 0.7270\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.5686 - acc: 0.7395 - val_loss: 0.5955 - val_acc: 0.7350\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.5540 - acc: 0.7445 - val_loss: 0.6343 - val_acc: 0.6830\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.5966 - acc: 0.7000 - val_loss: 0.5830 - val_acc: 0.7170\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5450 - acc: 0.7505 - val_loss: 0.5993 - val_acc: 0.7260\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5799 - acc: 0.7225 - val_loss: 0.5871 - val_acc: 0.7190\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.5432 - acc: 0.7490 - val_loss: 0.6063 - val_acc: 0.7170\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.5304 - acc: 0.7510 - val_loss: 0.5875 - val_acc: 0.7460\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.5767 - acc: 0.7040 - val_loss: 0.5839 - val_acc: 0.7280\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.5521 - acc: 0.7375 - val_loss: 0.5817 - val_acc: 0.7160\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.5543 - acc: 0.7395 - val_loss: 0.5855 - val_acc: 0.7230\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.5776 - acc: 0.7055 - val_loss: 0.6612 - val_acc: 0.6860\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.5549 - acc: 0.7360 - val_loss: 0.5888 - val_acc: 0.7190\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.5363 - acc: 0.7430 - val_loss: 0.6636 - val_acc: 0.6910\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.5585 - acc: 0.7270 - val_loss: 0.6154 - val_acc: 0.7100\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.5641 - acc: 0.7350 - val_loss: 0.5917 - val_acc: 0.7360\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.5426 - acc: 0.7485 - val_loss: 0.5833 - val_acc: 0.7240\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.5285 - acc: 0.7580 - val_loss: 0.5933 - val_acc: 0.7160\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.5252 - acc: 0.7480 - val_loss: 0.5970 - val_acc: 0.7380\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.6051 - acc: 0.7105 - val_loss: 0.6615 - val_acc: 0.6730\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5529 - acc: 0.7275 - val_loss: 0.6135 - val_acc: 0.7110\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.5363 - acc: 0.7485 - val_loss: 0.5720 - val_acc: 0.7330\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.5200 - acc: 0.7670 - val_loss: 0.5840 - val_acc: 0.7270\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5307 - acc: 0.7630 - val_loss: 0.5990 - val_acc: 0.7150\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.5546 - acc: 0.7305 - val_loss: 0.6394 - val_acc: 0.7010\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.5656 - acc: 0.7320 - val_loss: 0.5770 - val_acc: 0.7310\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.5287 - acc: 0.7540 - val_loss: 0.5947 - val_acc: 0.7210\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.5248 - acc: 0.7565 - val_loss: 0.6017 - val_acc: 0.7220\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 0.5832 - acc: 0.7200 - val_loss: 0.6090 - val_acc: 0.7110\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.5223 - acc: 0.7565 - val_loss: 0.6227 - val_acc: 0.7060\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.5247 - acc: 0.7530 - val_loss: 0.5988 - val_acc: 0.7340\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.5268 - acc: 0.7565 - val_loss: 0.5804 - val_acc: 0.7370\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 0.5498 - acc: 0.7135 - val_loss: 0.6022 - val_acc: 0.7280\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.5316 - acc: 0.7545 - val_loss: 0.6560 - val_acc: 0.6860\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.5335 - acc: 0.7470 - val_loss: 0.6084 - val_acc: 0.7440\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 0.5122 - acc: 0.7710 - val_loss: 0.5885 - val_acc: 0.7320\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.5247 - acc: 0.7640 - val_loss: 0.6229 - val_acc: 0.7030\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.5662 - acc: 0.7255 - val_loss: 0.5872 - val_acc: 0.7190\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5059 - acc: 0.7705 - val_loss: 0.6009 - val_acc: 0.7300\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.5305 - acc: 0.7545 - val_loss: 0.6036 - val_acc: 0.7310\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.5308 - acc: 0.7510 - val_loss: 0.6050 - val_acc: 0.7090\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.5289 - acc: 0.7475 - val_loss: 0.5764 - val_acc: 0.7430\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.4904 - acc: 0.7775 - val_loss: 0.6065 - val_acc: 0.7330\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.5605 - acc: 0.7300 - val_loss: 0.5984 - val_acc: 0.7280\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.5018 - acc: 0.7605 - val_loss: 0.5879 - val_acc: 0.7450\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.5363 - acc: 0.7485 - val_loss: 0.6106 - val_acc: 0.7070\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.5214 - acc: 0.7530 - val_loss: 0.6082 - val_acc: 0.7340\n",
      "Test loss: 0.6081847085952758\n",
      "Test accuracy: 0.734\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256*2, activation='relu', input_shape=(col-1,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256*2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "classes = model.predict(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.5622219e-01 9.6915674e-04 2.4280863e-01]\n",
      " [7.5979722e-01 1.1964265e-03 2.3900643e-01]\n",
      " [6.1479761e-05 6.1850911e-01 3.8142937e-01]\n",
      " ...\n",
      " [6.8936497e-01 2.1743931e-02 2.8889111e-01]\n",
      " [9.6748382e-01 6.1194744e-04 3.1904247e-02]\n",
      " [2.6304166e-05 6.0386640e-01 3.9610732e-01]]\n",
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(np.shape(classes)) #probability distribution for each of the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
