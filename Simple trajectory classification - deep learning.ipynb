{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and Result \n",
    "\n",
    "Using an appropriately tuned deep feed-forward neural network to train the data ($2000$ training examples, with each example consisting of a time series/trajectory for the $x$-coordinate of 102 steps), we found that accuracy on the test data ($1000$ test examples, with each example consisting of a trajectory for the $x$-coordinate of 102 steps) is around $77 \\%$ (a significant improvement over the accuracy of a random baseline). The accuracy rate could be further improved by using, for instance, more training data, including the trajectory for the $y$-coordinate in the training data set, or a more sophisticated neural network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# First, we get the data\n",
    "dataset0 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m0.csv\",header=None)\n",
    "dataset1 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m1.csv\",header=None)\n",
    "dataset2 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m2.csv\",header=None)\n",
    "\n",
    "li = []\n",
    "li.append(dataset0)\n",
    "li.append(dataset1)\n",
    "li.append(dataset2)\n",
    "\n",
    "dataset = np.array(pd.concat(li, axis=0, ignore_index=True))\n",
    "np.random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "np.savetxt('all_traj.csv', np.c_[dataset], delimiter=',') \n",
    "\n",
    "dataset.head(10)\n",
    "row=np.shape(dataset)[0]\n",
    "col=np.shape(dataset)[1]\n",
    "print(row); print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 102)\n",
      "(1000, 102)\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "num_train = int(row*2/3)\n",
    "training_set = dataset[:num_train].iloc[:,0:col].values\n",
    "test_set = dataset[num_train:].iloc[:,0:col].values\n",
    "print(np.shape(training_set)); print(np.shape(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.008112</td>\n",
       "      <td>-0.007560</td>\n",
       "      <td>-0.004491</td>\n",
       "      <td>-0.036412</td>\n",
       "      <td>-0.054547</td>\n",
       "      <td>-0.058271</td>\n",
       "      <td>-0.063641</td>\n",
       "      <td>-0.080891</td>\n",
       "      <td>-0.076679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496025</td>\n",
       "      <td>-0.478685</td>\n",
       "      <td>-0.501630</td>\n",
       "      <td>-0.511632</td>\n",
       "      <td>-0.513563</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.509612</td>\n",
       "      <td>-0.514935</td>\n",
       "      <td>-0.531241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-0.010614</td>\n",
       "      <td>-0.016995</td>\n",
       "      <td>-0.033122</td>\n",
       "      <td>-0.075458</td>\n",
       "      <td>-0.088546</td>\n",
       "      <td>-0.110400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389688</td>\n",
       "      <td>-0.378837</td>\n",
       "      <td>-0.390090</td>\n",
       "      <td>-0.366439</td>\n",
       "      <td>-0.368462</td>\n",
       "      <td>-0.367142</td>\n",
       "      <td>-0.366486</td>\n",
       "      <td>-0.399945</td>\n",
       "      <td>-0.385635</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005832</td>\n",
       "      <td>-0.014218</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>-0.014983</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>-0.021933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179858</td>\n",
       "      <td>0.172982</td>\n",
       "      <td>0.167911</td>\n",
       "      <td>0.171535</td>\n",
       "      <td>0.148475</td>\n",
       "      <td>0.134451</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>0.122040</td>\n",
       "      <td>0.107765</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025367</td>\n",
       "      <td>-0.029277</td>\n",
       "      <td>-0.015769</td>\n",
       "      <td>-0.031835</td>\n",
       "      <td>-0.037794</td>\n",
       "      <td>-0.038581</td>\n",
       "      <td>-0.032850</td>\n",
       "      <td>-0.031531</td>\n",
       "      <td>-0.025551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520201</td>\n",
       "      <td>-0.542546</td>\n",
       "      <td>-0.528929</td>\n",
       "      <td>-0.533476</td>\n",
       "      <td>-0.530742</td>\n",
       "      <td>-0.532826</td>\n",
       "      <td>-0.546026</td>\n",
       "      <td>-0.579208</td>\n",
       "      <td>-0.609824</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015284</td>\n",
       "      <td>-0.024569</td>\n",
       "      <td>-0.020933</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>-0.008785</td>\n",
       "      <td>-0.013706</td>\n",
       "      <td>0.014585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475318</td>\n",
       "      <td>-0.482259</td>\n",
       "      <td>-0.498864</td>\n",
       "      <td>-0.514436</td>\n",
       "      <td>-0.515569</td>\n",
       "      <td>-0.534107</td>\n",
       "      <td>-0.537033</td>\n",
       "      <td>-0.540476</td>\n",
       "      <td>-0.530733</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>-0.004999</td>\n",
       "      <td>-0.019461</td>\n",
       "      <td>-0.021444</td>\n",
       "      <td>-0.034118</td>\n",
       "      <td>-0.057695</td>\n",
       "      <td>-0.062441</td>\n",
       "      <td>-0.064673</td>\n",
       "      <td>-0.079475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.408875</td>\n",
       "      <td>-0.401754</td>\n",
       "      <td>-0.413367</td>\n",
       "      <td>-0.419506</td>\n",
       "      <td>-0.407817</td>\n",
       "      <td>-0.442742</td>\n",
       "      <td>-0.446983</td>\n",
       "      <td>-0.491275</td>\n",
       "      <td>-0.513882</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012345</td>\n",
       "      <td>-0.034076</td>\n",
       "      <td>-0.044309</td>\n",
       "      <td>-0.028916</td>\n",
       "      <td>-0.016485</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>-0.013393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>-0.013922</td>\n",
       "      <td>-0.014535</td>\n",
       "      <td>-0.003871</td>\n",
       "      <td>-0.031532</td>\n",
       "      <td>-0.028319</td>\n",
       "      <td>-0.033787</td>\n",
       "      <td>-0.013261</td>\n",
       "      <td>-0.020986</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>-0.012549</td>\n",
       "      <td>-0.007477</td>\n",
       "      <td>-0.016921</td>\n",
       "      <td>-0.010790</td>\n",
       "      <td>-0.024641</td>\n",
       "      <td>-0.016604</td>\n",
       "      <td>-0.014605</td>\n",
       "      <td>-0.031147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021434</td>\n",
       "      <td>0.033390</td>\n",
       "      <td>0.026988</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>-0.007462</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>-0.009042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030196</td>\n",
       "      <td>-0.040763</td>\n",
       "      <td>-0.045251</td>\n",
       "      <td>-0.062796</td>\n",
       "      <td>-0.064600</td>\n",
       "      <td>-0.049705</td>\n",
       "      <td>-0.084959</td>\n",
       "      <td>-0.123545</td>\n",
       "      <td>-0.122448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531611</td>\n",
       "      <td>-0.538016</td>\n",
       "      <td>-0.529875</td>\n",
       "      <td>-0.545023</td>\n",
       "      <td>-0.553975</td>\n",
       "      <td>-0.566684</td>\n",
       "      <td>-0.564694</td>\n",
       "      <td>-0.558005</td>\n",
       "      <td>-0.575355</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>0.027802</td>\n",
       "      <td>0.044385</td>\n",
       "      <td>0.044554</td>\n",
       "      <td>0.038671</td>\n",
       "      <td>0.038163</td>\n",
       "      <td>0.044501</td>\n",
       "      <td>0.025458</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468212</td>\n",
       "      <td>-0.499556</td>\n",
       "      <td>-0.509951</td>\n",
       "      <td>-0.523403</td>\n",
       "      <td>-0.518249</td>\n",
       "      <td>-0.531277</td>\n",
       "      <td>-0.522842</td>\n",
       "      <td>-0.541226</td>\n",
       "      <td>-0.554423</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0  0.0 -0.008112 -0.007560 -0.004491 -0.036412 -0.054547 -0.058271 -0.063641   \n",
       "1  0.0  0.001431  0.006314  0.000071 -0.010614 -0.016995 -0.033122 -0.075458   \n",
       "2  0.0 -0.005832 -0.014218  0.009856  0.009080  0.009145  0.003475 -0.014983   \n",
       "3  0.0 -0.025367 -0.029277 -0.015769 -0.031835 -0.037794 -0.038581 -0.032850   \n",
       "4  0.0 -0.015284 -0.024569 -0.020933 -0.014208  0.011406  0.010233 -0.008785   \n",
       "5  0.0  0.005596 -0.004999 -0.019461 -0.021444 -0.034118 -0.057695 -0.062441   \n",
       "6  0.0 -0.012345 -0.034076 -0.044309 -0.028916 -0.016485 -0.016571  0.002102   \n",
       "7  0.0  0.001046 -0.012549 -0.007477 -0.016921 -0.010790 -0.024641 -0.016604   \n",
       "8  0.0 -0.030196 -0.040763 -0.045251 -0.062796 -0.064600 -0.049705 -0.084959   \n",
       "9  0.0 -0.001540  0.027802  0.044385  0.044554  0.038671  0.038163  0.044501   \n",
       "\n",
       "        8         9    ...       92        93        94        95        96   \\\n",
       "0 -0.080891 -0.076679  ... -0.496025 -0.478685 -0.501630 -0.511632 -0.513563   \n",
       "1 -0.088546 -0.110400  ... -0.389688 -0.378837 -0.390090 -0.366439 -0.368462   \n",
       "2 -0.021645 -0.021933  ...  0.179858  0.172982  0.167911  0.171535  0.148475   \n",
       "3 -0.031531 -0.025551  ... -0.520201 -0.542546 -0.528929 -0.533476 -0.530742   \n",
       "4 -0.013706  0.014585  ... -0.475318 -0.482259 -0.498864 -0.514436 -0.515569   \n",
       "5 -0.064673 -0.079475  ... -0.408875 -0.401754 -0.413367 -0.419506 -0.407817   \n",
       "6  0.012238 -0.013393  ... -0.002833 -0.013922 -0.014535 -0.003871 -0.031532   \n",
       "7 -0.014605 -0.031147  ...  0.021434  0.033390  0.026988  0.031465  0.008578   \n",
       "8 -0.123545 -0.122448  ... -0.531611 -0.538016 -0.529875 -0.545023 -0.553975   \n",
       "9  0.025458 -0.006539  ... -0.468212 -0.499556 -0.509951 -0.523403 -0.518249   \n",
       "\n",
       "        97        98        99        100  101  \n",
       "0 -0.527046 -0.509612 -0.514935 -0.531241  1.0  \n",
       "1 -0.367142 -0.366486 -0.399945 -0.385635  1.0  \n",
       "2  0.134451  0.119498  0.122040  0.107765  2.0  \n",
       "3 -0.532826 -0.546026 -0.579208 -0.609824  2.0  \n",
       "4 -0.534107 -0.537033 -0.540476 -0.530733  2.0  \n",
       "5 -0.442742 -0.446983 -0.491275 -0.513882  2.0  \n",
       "6 -0.028319 -0.033787 -0.013261 -0.020986  2.0  \n",
       "7  0.002180 -0.007462  0.003261 -0.009042  0.0  \n",
       "8 -0.566684 -0.564694 -0.558005 -0.575355  2.0  \n",
       "9 -0.531277 -0.522842 -0.541226 -0.554423  1.0  \n",
       "\n",
       "[10 rows x 102 columns]"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.DataFrame(training_set)\n",
    "training_df.head(10)\n",
    "\n",
    "#pd.plotting.scatter_matrix(training_df, c=training_set['750'], figsize=(15,15), marker='o', s=60)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.338"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy of a random baseline\n",
    "import copy\n",
    "y_test_copy=copy.copy(y_test)\n",
    "np.random.shuffle(y_test_copy)\n",
    "float(np.sum(np.array(y_test)==np.array(y_test_copy)))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 train samples\n",
      "1000 test samples\n",
      "(2000, 101)\n",
      "(1000, 101)\n",
      "(2000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "#from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 500\n",
    "epochs = 800\n",
    "\n",
    "#number of categories\n",
    "num_classes = 3\n",
    "\n",
    "# the data, split between train and test sets\n",
    "x_train=training_set[:,:col-1]\n",
    "y_train=training_set[:,col-1:]\n",
    "x_test=test_set[:,:col-1]\n",
    "y_test=test_set[:,col-1:]\n",
    "\n",
    "x_train = x_train.reshape(num_train, col-1)\n",
    "x_test = x_test.reshape(row-num_train, col-1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(np.shape(x_train)); print(np.shape(x_test))\n",
    "print(np.shape(y_train)); print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_169 (Dense)            (None, 128)               13056     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 29,955\n",
      "Trainable params: 29,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/800\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 1.1615 - acc: 0.3470 - val_loss: 1.0624 - val_acc: 0.3550\n",
      "Epoch 2/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 1.0895 - acc: 0.4180 - val_loss: 1.0045 - val_acc: 0.3500\n",
      "Epoch 3/800\n",
      "2000/2000 [==============================] - 0s 56us/step - loss: 1.0452 - acc: 0.4580 - val_loss: 0.9501 - val_acc: 0.7100\n",
      "Epoch 4/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.9961 - acc: 0.4885 - val_loss: 0.8992 - val_acc: 0.6940\n",
      "Epoch 5/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.9430 - acc: 0.5435 - val_loss: 0.8574 - val_acc: 0.7240\n",
      "Epoch 6/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.9135 - acc: 0.5400 - val_loss: 0.8107 - val_acc: 0.7220\n",
      "Epoch 7/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.8675 - acc: 0.5625 - val_loss: 0.7712 - val_acc: 0.7090\n",
      "Epoch 8/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.8166 - acc: 0.6095 - val_loss: 0.7394 - val_acc: 0.7070\n",
      "Epoch 9/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.8087 - acc: 0.6025 - val_loss: 0.7150 - val_acc: 0.7320\n",
      "Epoch 10/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.7860 - acc: 0.6105 - val_loss: 0.7047 - val_acc: 0.7350\n",
      "Epoch 11/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.7634 - acc: 0.6300 - val_loss: 0.6961 - val_acc: 0.7150\n",
      "Epoch 12/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.7468 - acc: 0.6160 - val_loss: 0.6675 - val_acc: 0.7320\n",
      "Epoch 13/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.7301 - acc: 0.6245 - val_loss: 0.6571 - val_acc: 0.7290\n",
      "Epoch 14/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.7191 - acc: 0.6480 - val_loss: 0.6588 - val_acc: 0.7310\n",
      "Epoch 15/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.7165 - acc: 0.6585 - val_loss: 0.6518 - val_acc: 0.7290\n",
      "Epoch 16/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.7096 - acc: 0.6485 - val_loss: 0.6377 - val_acc: 0.7320\n",
      "Epoch 17/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.7096 - acc: 0.6560 - val_loss: 0.6341 - val_acc: 0.7350\n",
      "Epoch 18/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.7058 - acc: 0.6470 - val_loss: 0.6324 - val_acc: 0.7390\n",
      "Epoch 19/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6908 - acc: 0.6505 - val_loss: 0.6258 - val_acc: 0.7350\n",
      "Epoch 20/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6842 - acc: 0.6760 - val_loss: 0.6292 - val_acc: 0.7400\n",
      "Epoch 21/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6890 - acc: 0.6700 - val_loss: 0.6190 - val_acc: 0.7350\n",
      "Epoch 22/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6793 - acc: 0.6795 - val_loss: 0.6224 - val_acc: 0.7350\n",
      "Epoch 23/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6904 - acc: 0.6615 - val_loss: 0.6114 - val_acc: 0.7360\n",
      "Epoch 24/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.6780 - acc: 0.6850 - val_loss: 0.6147 - val_acc: 0.7330\n",
      "Epoch 25/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6778 - acc: 0.6760 - val_loss: 0.6121 - val_acc: 0.7380\n",
      "Epoch 26/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6589 - acc: 0.6950 - val_loss: 0.6074 - val_acc: 0.7340\n",
      "Epoch 27/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.6800 - acc: 0.6715 - val_loss: 0.6160 - val_acc: 0.7300\n",
      "Epoch 28/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6758 - acc: 0.6685 - val_loss: 0.6094 - val_acc: 0.7390\n",
      "Epoch 29/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6658 - acc: 0.6830 - val_loss: 0.6043 - val_acc: 0.7350\n",
      "Epoch 30/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6671 - acc: 0.6950 - val_loss: 0.6072 - val_acc: 0.7400\n",
      "Epoch 31/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.6627 - acc: 0.6865 - val_loss: 0.5987 - val_acc: 0.7320\n",
      "Epoch 32/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.6671 - acc: 0.6850 - val_loss: 0.6023 - val_acc: 0.7380\n",
      "Epoch 33/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6614 - acc: 0.6985 - val_loss: 0.6014 - val_acc: 0.7390\n",
      "Epoch 34/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6587 - acc: 0.6910 - val_loss: 0.6015 - val_acc: 0.7340\n",
      "Epoch 35/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6569 - acc: 0.6925 - val_loss: 0.6022 - val_acc: 0.7380\n",
      "Epoch 36/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6618 - acc: 0.6860 - val_loss: 0.5948 - val_acc: 0.7370\n",
      "Epoch 37/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6510 - acc: 0.6950 - val_loss: 0.5956 - val_acc: 0.7360\n",
      "Epoch 38/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6556 - acc: 0.6895 - val_loss: 0.5986 - val_acc: 0.7360\n",
      "Epoch 39/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6437 - acc: 0.6980 - val_loss: 0.5944 - val_acc: 0.7390\n",
      "Epoch 40/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6522 - acc: 0.6965 - val_loss: 0.5932 - val_acc: 0.7390\n",
      "Epoch 41/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6490 - acc: 0.6920 - val_loss: 0.5899 - val_acc: 0.7390\n",
      "Epoch 42/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6506 - acc: 0.6975 - val_loss: 0.6008 - val_acc: 0.7380\n",
      "Epoch 43/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6408 - acc: 0.6920 - val_loss: 0.5899 - val_acc: 0.7380\n",
      "Epoch 44/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6408 - acc: 0.7040 - val_loss: 0.5893 - val_acc: 0.7410\n",
      "Epoch 45/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.6423 - acc: 0.7015 - val_loss: 0.5887 - val_acc: 0.7390\n",
      "Epoch 46/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6372 - acc: 0.7010 - val_loss: 0.6072 - val_acc: 0.7330\n",
      "Epoch 47/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6338 - acc: 0.7060 - val_loss: 0.5913 - val_acc: 0.7430\n",
      "Epoch 48/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6484 - acc: 0.6930 - val_loss: 0.5844 - val_acc: 0.7380\n",
      "Epoch 49/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6338 - acc: 0.7015 - val_loss: 0.5859 - val_acc: 0.7400\n",
      "Epoch 50/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6490 - acc: 0.6995 - val_loss: 0.5895 - val_acc: 0.7350\n",
      "Epoch 51/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6353 - acc: 0.7105 - val_loss: 0.5842 - val_acc: 0.7390\n",
      "Epoch 52/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.6388 - acc: 0.7050 - val_loss: 0.5837 - val_acc: 0.7420\n",
      "Epoch 53/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.6377 - acc: 0.6935 - val_loss: 0.5843 - val_acc: 0.7360\n",
      "Epoch 54/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.6357 - acc: 0.7015 - val_loss: 0.5866 - val_acc: 0.7390\n",
      "Epoch 55/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.6361 - acc: 0.7030 - val_loss: 0.5811 - val_acc: 0.7360\n",
      "Epoch 56/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6396 - acc: 0.7100 - val_loss: 0.5920 - val_acc: 0.7410\n",
      "Epoch 57/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6344 - acc: 0.7035 - val_loss: 0.5885 - val_acc: 0.7420\n",
      "Epoch 58/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6358 - acc: 0.6985 - val_loss: 0.5913 - val_acc: 0.7410\n",
      "Epoch 59/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6287 - acc: 0.7060 - val_loss: 0.5823 - val_acc: 0.7420\n",
      "Epoch 60/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6333 - acc: 0.7025 - val_loss: 0.5897 - val_acc: 0.7420\n",
      "Epoch 61/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6318 - acc: 0.7090 - val_loss: 0.5850 - val_acc: 0.7380\n",
      "Epoch 62/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6293 - acc: 0.7060 - val_loss: 0.5887 - val_acc: 0.7420\n",
      "Epoch 63/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6288 - acc: 0.7050 - val_loss: 0.5873 - val_acc: 0.7440\n",
      "Epoch 64/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6280 - acc: 0.7090 - val_loss: 0.5822 - val_acc: 0.7400\n",
      "Epoch 65/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6336 - acc: 0.7145 - val_loss: 0.5920 - val_acc: 0.7420\n",
      "Epoch 66/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6281 - acc: 0.7075 - val_loss: 0.5901 - val_acc: 0.7400\n",
      "Epoch 67/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.6391 - acc: 0.7085 - val_loss: 0.5841 - val_acc: 0.7460\n",
      "Epoch 68/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6295 - acc: 0.7085 - val_loss: 0.5922 - val_acc: 0.7390\n",
      "Epoch 69/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.6349 - acc: 0.7080 - val_loss: 0.5840 - val_acc: 0.7460\n",
      "Epoch 70/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6256 - acc: 0.7115 - val_loss: 0.5844 - val_acc: 0.7430\n",
      "Epoch 71/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6283 - acc: 0.7110 - val_loss: 0.5863 - val_acc: 0.7440\n",
      "Epoch 72/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6295 - acc: 0.7130 - val_loss: 0.5902 - val_acc: 0.7390\n",
      "Epoch 73/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.6252 - acc: 0.7095 - val_loss: 0.5843 - val_acc: 0.7360\n",
      "Epoch 74/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6221 - acc: 0.7205 - val_loss: 0.5804 - val_acc: 0.7400\n",
      "Epoch 75/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6262 - acc: 0.7150 - val_loss: 0.5807 - val_acc: 0.7420\n",
      "Epoch 76/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.6287 - acc: 0.7045 - val_loss: 0.5796 - val_acc: 0.7420\n",
      "Epoch 77/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6285 - acc: 0.7145 - val_loss: 0.5843 - val_acc: 0.7460\n",
      "Epoch 78/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.6186 - acc: 0.7085 - val_loss: 0.5785 - val_acc: 0.7430\n",
      "Epoch 79/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6195 - acc: 0.7085 - val_loss: 0.5836 - val_acc: 0.7470\n",
      "Epoch 80/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6251 - acc: 0.7090 - val_loss: 0.5809 - val_acc: 0.7390\n",
      "Epoch 81/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6264 - acc: 0.7070 - val_loss: 0.5838 - val_acc: 0.7440\n",
      "Epoch 82/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6250 - acc: 0.7065 - val_loss: 0.5931 - val_acc: 0.7350\n",
      "Epoch 83/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6278 - acc: 0.7010 - val_loss: 0.5795 - val_acc: 0.7380\n",
      "Epoch 84/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6281 - acc: 0.7055 - val_loss: 0.5800 - val_acc: 0.7380\n",
      "Epoch 85/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6185 - acc: 0.7125 - val_loss: 0.5781 - val_acc: 0.7400\n",
      "Epoch 86/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6274 - acc: 0.7175 - val_loss: 0.5790 - val_acc: 0.7420\n",
      "Epoch 87/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6215 - acc: 0.7130 - val_loss: 0.5803 - val_acc: 0.7460\n",
      "Epoch 88/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.6238 - acc: 0.7085 - val_loss: 0.5773 - val_acc: 0.7410\n",
      "Epoch 89/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6211 - acc: 0.7125 - val_loss: 0.5778 - val_acc: 0.7390\n",
      "Epoch 90/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.6245 - acc: 0.7090 - val_loss: 0.5802 - val_acc: 0.7390\n",
      "Epoch 91/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6193 - acc: 0.7115 - val_loss: 0.5839 - val_acc: 0.7420\n",
      "Epoch 92/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6220 - acc: 0.7120 - val_loss: 0.5821 - val_acc: 0.7450\n",
      "Epoch 93/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6199 - acc: 0.7140 - val_loss: 0.5776 - val_acc: 0.7450\n",
      "Epoch 94/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6188 - acc: 0.7175 - val_loss: 0.5800 - val_acc: 0.7380\n",
      "Epoch 95/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6144 - acc: 0.7140 - val_loss: 0.5742 - val_acc: 0.7360\n",
      "Epoch 96/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6262 - acc: 0.7110 - val_loss: 0.5815 - val_acc: 0.7470\n",
      "Epoch 97/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6203 - acc: 0.7120 - val_loss: 0.5813 - val_acc: 0.7450\n",
      "Epoch 98/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.6182 - acc: 0.7160 - val_loss: 0.5758 - val_acc: 0.7410\n",
      "Epoch 99/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6208 - acc: 0.7150 - val_loss: 0.5771 - val_acc: 0.7440\n",
      "Epoch 100/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6184 - acc: 0.7115 - val_loss: 0.5799 - val_acc: 0.7420\n",
      "Epoch 101/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6185 - acc: 0.7150 - val_loss: 0.5752 - val_acc: 0.7400\n",
      "Epoch 102/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6197 - acc: 0.7180 - val_loss: 0.5791 - val_acc: 0.7350\n",
      "Epoch 103/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6219 - acc: 0.7110 - val_loss: 0.5797 - val_acc: 0.7440\n",
      "Epoch 104/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6154 - acc: 0.7120 - val_loss: 0.5778 - val_acc: 0.7410\n",
      "Epoch 105/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.6207 - acc: 0.7170 - val_loss: 0.5780 - val_acc: 0.7360\n",
      "Epoch 106/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6190 - acc: 0.7130 - val_loss: 0.5744 - val_acc: 0.7370\n",
      "Epoch 107/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6130 - acc: 0.7185 - val_loss: 0.5799 - val_acc: 0.7360\n",
      "Epoch 108/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6247 - acc: 0.7135 - val_loss: 0.5751 - val_acc: 0.7420\n",
      "Epoch 109/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6189 - acc: 0.7155 - val_loss: 0.5816 - val_acc: 0.7430\n",
      "Epoch 110/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6191 - acc: 0.7100 - val_loss: 0.5778 - val_acc: 0.7400\n",
      "Epoch 111/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.6141 - acc: 0.7170 - val_loss: 0.5769 - val_acc: 0.7460\n",
      "Epoch 112/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6171 - acc: 0.7175 - val_loss: 0.5759 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6112 - acc: 0.7140 - val_loss: 0.5788 - val_acc: 0.7440\n",
      "Epoch 114/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6127 - acc: 0.7175 - val_loss: 0.5815 - val_acc: 0.7440\n",
      "Epoch 115/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6195 - acc: 0.7095 - val_loss: 0.5743 - val_acc: 0.7370\n",
      "Epoch 116/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6129 - acc: 0.7065 - val_loss: 0.5787 - val_acc: 0.7410\n",
      "Epoch 117/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6092 - acc: 0.7180 - val_loss: 0.5782 - val_acc: 0.7400\n",
      "Epoch 118/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6116 - acc: 0.7175 - val_loss: 0.5759 - val_acc: 0.7400\n",
      "Epoch 119/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6113 - acc: 0.7165 - val_loss: 0.5780 - val_acc: 0.7440\n",
      "Epoch 120/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6122 - acc: 0.7205 - val_loss: 0.5763 - val_acc: 0.7400\n",
      "Epoch 121/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6136 - acc: 0.7160 - val_loss: 0.5736 - val_acc: 0.7400\n",
      "Epoch 122/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6142 - acc: 0.7110 - val_loss: 0.5709 - val_acc: 0.7440\n",
      "Epoch 123/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6094 - acc: 0.7140 - val_loss: 0.5722 - val_acc: 0.7390\n",
      "Epoch 124/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6126 - acc: 0.7180 - val_loss: 0.5817 - val_acc: 0.7430\n",
      "Epoch 125/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6115 - acc: 0.7190 - val_loss: 0.5730 - val_acc: 0.7400\n",
      "Epoch 126/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6092 - acc: 0.7160 - val_loss: 0.5724 - val_acc: 0.7440\n",
      "Epoch 127/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6114 - acc: 0.7185 - val_loss: 0.5791 - val_acc: 0.7400\n",
      "Epoch 128/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6115 - acc: 0.7130 - val_loss: 0.5716 - val_acc: 0.7350\n",
      "Epoch 129/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6114 - acc: 0.7090 - val_loss: 0.5757 - val_acc: 0.7360\n",
      "Epoch 130/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6133 - acc: 0.7200 - val_loss: 0.5756 - val_acc: 0.7400\n",
      "Epoch 131/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6116 - acc: 0.7100 - val_loss: 0.5767 - val_acc: 0.7430\n",
      "Epoch 132/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6119 - acc: 0.7125 - val_loss: 0.5782 - val_acc: 0.7390\n",
      "Epoch 133/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6093 - acc: 0.7110 - val_loss: 0.5765 - val_acc: 0.7400\n",
      "Epoch 134/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6130 - acc: 0.7225 - val_loss: 0.5712 - val_acc: 0.7390\n",
      "Epoch 135/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6100 - acc: 0.7150 - val_loss: 0.5711 - val_acc: 0.7420\n",
      "Epoch 136/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.6129 - acc: 0.7165 - val_loss: 0.5738 - val_acc: 0.7390\n",
      "Epoch 137/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6082 - acc: 0.7140 - val_loss: 0.5728 - val_acc: 0.7410\n",
      "Epoch 138/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.6082 - acc: 0.7160 - val_loss: 0.5756 - val_acc: 0.7370\n",
      "Epoch 139/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6061 - acc: 0.7215 - val_loss: 0.5728 - val_acc: 0.7380\n",
      "Epoch 140/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6123 - acc: 0.7155 - val_loss: 0.5738 - val_acc: 0.7350\n",
      "Epoch 141/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6071 - acc: 0.7180 - val_loss: 0.5705 - val_acc: 0.7410\n",
      "Epoch 142/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6098 - acc: 0.7170 - val_loss: 0.5715 - val_acc: 0.7380\n",
      "Epoch 143/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6061 - acc: 0.7185 - val_loss: 0.5694 - val_acc: 0.7360\n",
      "Epoch 144/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6100 - acc: 0.7120 - val_loss: 0.5737 - val_acc: 0.7440\n",
      "Epoch 145/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6111 - acc: 0.7150 - val_loss: 0.5719 - val_acc: 0.7410\n",
      "Epoch 146/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6085 - acc: 0.7140 - val_loss: 0.5750 - val_acc: 0.7440\n",
      "Epoch 147/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6050 - acc: 0.7210 - val_loss: 0.5706 - val_acc: 0.7380\n",
      "Epoch 148/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6008 - acc: 0.7190 - val_loss: 0.5695 - val_acc: 0.7420\n",
      "Epoch 149/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6099 - acc: 0.7130 - val_loss: 0.5697 - val_acc: 0.7350\n",
      "Epoch 150/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6064 - acc: 0.7170 - val_loss: 0.5796 - val_acc: 0.7440\n",
      "Epoch 151/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6046 - acc: 0.7185 - val_loss: 0.5732 - val_acc: 0.7350\n",
      "Epoch 152/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6051 - acc: 0.7135 - val_loss: 0.5698 - val_acc: 0.7370\n",
      "Epoch 153/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6090 - acc: 0.7195 - val_loss: 0.5756 - val_acc: 0.7390\n",
      "Epoch 154/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6059 - acc: 0.7130 - val_loss: 0.5726 - val_acc: 0.7360\n",
      "Epoch 155/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6055 - acc: 0.7115 - val_loss: 0.5712 - val_acc: 0.7420\n",
      "Epoch 156/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6079 - acc: 0.7115 - val_loss: 0.5740 - val_acc: 0.7430\n",
      "Epoch 157/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6022 - acc: 0.7175 - val_loss: 0.5704 - val_acc: 0.7410\n",
      "Epoch 158/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6053 - acc: 0.7200 - val_loss: 0.5719 - val_acc: 0.7400\n",
      "Epoch 159/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6029 - acc: 0.7190 - val_loss: 0.5710 - val_acc: 0.7420\n",
      "Epoch 160/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6042 - acc: 0.7165 - val_loss: 0.5702 - val_acc: 0.7310\n",
      "Epoch 161/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6032 - acc: 0.7230 - val_loss: 0.5738 - val_acc: 0.7380\n",
      "Epoch 162/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6025 - acc: 0.7165 - val_loss: 0.5702 - val_acc: 0.7400\n",
      "Epoch 163/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6025 - acc: 0.7200 - val_loss: 0.5669 - val_acc: 0.7430\n",
      "Epoch 164/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6041 - acc: 0.7215 - val_loss: 0.5761 - val_acc: 0.7420\n",
      "Epoch 165/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6064 - acc: 0.7115 - val_loss: 0.5705 - val_acc: 0.7420\n",
      "Epoch 166/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6035 - acc: 0.7085 - val_loss: 0.5706 - val_acc: 0.7430\n",
      "Epoch 167/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6056 - acc: 0.7130 - val_loss: 0.5682 - val_acc: 0.7390\n",
      "Epoch 168/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5954 - acc: 0.7155 - val_loss: 0.5728 - val_acc: 0.7370\n",
      "Epoch 169/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6001 - acc: 0.7235 - val_loss: 0.5691 - val_acc: 0.7450\n",
      "Epoch 170/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6036 - acc: 0.7185 - val_loss: 0.5694 - val_acc: 0.7330\n",
      "Epoch 171/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6053 - acc: 0.7155 - val_loss: 0.5671 - val_acc: 0.7400\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6080 - acc: 0.7105 - val_loss: 0.5715 - val_acc: 0.7370\n",
      "Epoch 173/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6060 - acc: 0.7135 - val_loss: 0.5696 - val_acc: 0.7440\n",
      "Epoch 174/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6035 - acc: 0.7200 - val_loss: 0.5718 - val_acc: 0.7360\n",
      "Epoch 175/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6039 - acc: 0.7090 - val_loss: 0.5670 - val_acc: 0.7410\n",
      "Epoch 176/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5974 - acc: 0.7195 - val_loss: 0.5712 - val_acc: 0.7440\n",
      "Epoch 177/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.6055 - acc: 0.7180 - val_loss: 0.5705 - val_acc: 0.7370\n",
      "Epoch 178/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.6029 - acc: 0.7160 - val_loss: 0.5681 - val_acc: 0.7330\n",
      "Epoch 179/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.6060 - acc: 0.7085 - val_loss: 0.5715 - val_acc: 0.7370\n",
      "Epoch 180/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5984 - acc: 0.7155 - val_loss: 0.5675 - val_acc: 0.7350\n",
      "Epoch 181/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6014 - acc: 0.7130 - val_loss: 0.5646 - val_acc: 0.7360\n",
      "Epoch 182/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6046 - acc: 0.7190 - val_loss: 0.5684 - val_acc: 0.7420\n",
      "Epoch 183/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6001 - acc: 0.7205 - val_loss: 0.5648 - val_acc: 0.7380\n",
      "Epoch 184/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5998 - acc: 0.7100 - val_loss: 0.5665 - val_acc: 0.7400\n",
      "Epoch 185/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.6007 - acc: 0.7090 - val_loss: 0.5655 - val_acc: 0.7340\n",
      "Epoch 186/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5967 - acc: 0.7220 - val_loss: 0.5641 - val_acc: 0.7390\n",
      "Epoch 187/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6059 - acc: 0.7100 - val_loss: 0.5653 - val_acc: 0.7300\n",
      "Epoch 188/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5964 - acc: 0.7165 - val_loss: 0.5664 - val_acc: 0.7380\n",
      "Epoch 189/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5982 - acc: 0.7205 - val_loss: 0.5624 - val_acc: 0.7350\n",
      "Epoch 190/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5995 - acc: 0.7170 - val_loss: 0.5685 - val_acc: 0.7380\n",
      "Epoch 191/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.6018 - acc: 0.7145 - val_loss: 0.5649 - val_acc: 0.7340\n",
      "Epoch 192/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5983 - acc: 0.7115 - val_loss: 0.5622 - val_acc: 0.7380\n",
      "Epoch 193/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5965 - acc: 0.7155 - val_loss: 0.5627 - val_acc: 0.7390\n",
      "Epoch 194/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6050 - acc: 0.7035 - val_loss: 0.5639 - val_acc: 0.7340\n",
      "Epoch 195/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5906 - acc: 0.7240 - val_loss: 0.5626 - val_acc: 0.7330\n",
      "Epoch 196/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5978 - acc: 0.7185 - val_loss: 0.5645 - val_acc: 0.7350\n",
      "Epoch 197/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5944 - acc: 0.7125 - val_loss: 0.5669 - val_acc: 0.7480\n",
      "Epoch 198/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5981 - acc: 0.7200 - val_loss: 0.5621 - val_acc: 0.7340\n",
      "Epoch 199/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5968 - acc: 0.7160 - val_loss: 0.5616 - val_acc: 0.7310\n",
      "Epoch 200/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5869 - acc: 0.7190 - val_loss: 0.5654 - val_acc: 0.7390\n",
      "Epoch 201/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5987 - acc: 0.7200 - val_loss: 0.5604 - val_acc: 0.7410\n",
      "Epoch 202/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5943 - acc: 0.7160 - val_loss: 0.5569 - val_acc: 0.7330\n",
      "Epoch 203/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5935 - acc: 0.7130 - val_loss: 0.5612 - val_acc: 0.7370\n",
      "Epoch 204/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5969 - acc: 0.7195 - val_loss: 0.5576 - val_acc: 0.7320\n",
      "Epoch 205/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5964 - acc: 0.7165 - val_loss: 0.5605 - val_acc: 0.7390\n",
      "Epoch 206/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5957 - acc: 0.7120 - val_loss: 0.5603 - val_acc: 0.7330\n",
      "Epoch 207/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5899 - acc: 0.7195 - val_loss: 0.5558 - val_acc: 0.7330\n",
      "Epoch 208/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5945 - acc: 0.7125 - val_loss: 0.5592 - val_acc: 0.7330\n",
      "Epoch 209/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5885 - acc: 0.7180 - val_loss: 0.5642 - val_acc: 0.7370\n",
      "Epoch 210/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5953 - acc: 0.7130 - val_loss: 0.5598 - val_acc: 0.7330\n",
      "Epoch 211/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5917 - acc: 0.7180 - val_loss: 0.5559 - val_acc: 0.7270\n",
      "Epoch 212/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5894 - acc: 0.7195 - val_loss: 0.5581 - val_acc: 0.7400\n",
      "Epoch 213/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5901 - acc: 0.7225 - val_loss: 0.5620 - val_acc: 0.7370\n",
      "Epoch 214/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5938 - acc: 0.7150 - val_loss: 0.5604 - val_acc: 0.7450\n",
      "Epoch 215/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5948 - acc: 0.7155 - val_loss: 0.5629 - val_acc: 0.7450\n",
      "Epoch 216/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5903 - acc: 0.7110 - val_loss: 0.5557 - val_acc: 0.7290\n",
      "Epoch 217/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5892 - acc: 0.7210 - val_loss: 0.5571 - val_acc: 0.7360\n",
      "Epoch 218/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5890 - acc: 0.7235 - val_loss: 0.5571 - val_acc: 0.7370\n",
      "Epoch 219/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5909 - acc: 0.7220 - val_loss: 0.5567 - val_acc: 0.7320\n",
      "Epoch 220/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5858 - acc: 0.7180 - val_loss: 0.5534 - val_acc: 0.7330\n",
      "Epoch 221/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5861 - acc: 0.7185 - val_loss: 0.5533 - val_acc: 0.7360\n",
      "Epoch 222/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5854 - acc: 0.7150 - val_loss: 0.5512 - val_acc: 0.7390\n",
      "Epoch 223/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5804 - acc: 0.7235 - val_loss: 0.5502 - val_acc: 0.7340\n",
      "Epoch 224/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5853 - acc: 0.7180 - val_loss: 0.5540 - val_acc: 0.7340\n",
      "Epoch 225/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5825 - acc: 0.7185 - val_loss: 0.5568 - val_acc: 0.7350\n",
      "Epoch 226/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5912 - acc: 0.7115 - val_loss: 0.5491 - val_acc: 0.7400\n",
      "Epoch 227/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5800 - acc: 0.7240 - val_loss: 0.5506 - val_acc: 0.7360\n",
      "Epoch 228/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5814 - acc: 0.7200 - val_loss: 0.5600 - val_acc: 0.7470\n",
      "Epoch 229/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5822 - acc: 0.7285 - val_loss: 0.5470 - val_acc: 0.7300\n",
      "Epoch 230/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5870 - acc: 0.7265 - val_loss: 0.5528 - val_acc: 0.7360\n",
      "Epoch 231/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5826 - acc: 0.7265 - val_loss: 0.5508 - val_acc: 0.7410\n",
      "Epoch 232/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5861 - acc: 0.7270 - val_loss: 0.5490 - val_acc: 0.7370\n",
      "Epoch 233/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5815 - acc: 0.7240 - val_loss: 0.5486 - val_acc: 0.7460\n",
      "Epoch 234/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5839 - acc: 0.7295 - val_loss: 0.5497 - val_acc: 0.7460\n",
      "Epoch 235/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5756 - acc: 0.7325 - val_loss: 0.5475 - val_acc: 0.7370\n",
      "Epoch 236/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5766 - acc: 0.7255 - val_loss: 0.5482 - val_acc: 0.7440\n",
      "Epoch 237/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5848 - acc: 0.7340 - val_loss: 0.5451 - val_acc: 0.7360\n",
      "Epoch 238/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5832 - acc: 0.7250 - val_loss: 0.5455 - val_acc: 0.7460\n",
      "Epoch 239/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5768 - acc: 0.7335 - val_loss: 0.5488 - val_acc: 0.7560\n",
      "Epoch 240/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5753 - acc: 0.7400 - val_loss: 0.5472 - val_acc: 0.7420\n",
      "Epoch 241/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5731 - acc: 0.7250 - val_loss: 0.5439 - val_acc: 0.7440\n",
      "Epoch 242/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5887 - acc: 0.7225 - val_loss: 0.5442 - val_acc: 0.7420\n",
      "Epoch 243/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5807 - acc: 0.7335 - val_loss: 0.5452 - val_acc: 0.7550\n",
      "Epoch 244/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5866 - acc: 0.7265 - val_loss: 0.5410 - val_acc: 0.7480\n",
      "Epoch 245/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5801 - acc: 0.7300 - val_loss: 0.5436 - val_acc: 0.7520\n",
      "Epoch 246/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5752 - acc: 0.7305 - val_loss: 0.5429 - val_acc: 0.7450\n",
      "Epoch 247/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5686 - acc: 0.7365 - val_loss: 0.5449 - val_acc: 0.7500\n",
      "Epoch 248/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5783 - acc: 0.7325 - val_loss: 0.5407 - val_acc: 0.7470\n",
      "Epoch 249/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5798 - acc: 0.7260 - val_loss: 0.5461 - val_acc: 0.7450\n",
      "Epoch 250/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5824 - acc: 0.7310 - val_loss: 0.5447 - val_acc: 0.7530\n",
      "Epoch 251/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5710 - acc: 0.7425 - val_loss: 0.5426 - val_acc: 0.7480\n",
      "Epoch 252/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5747 - acc: 0.7305 - val_loss: 0.5433 - val_acc: 0.7480\n",
      "Epoch 253/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5723 - acc: 0.7350 - val_loss: 0.5396 - val_acc: 0.7500\n",
      "Epoch 254/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5738 - acc: 0.7400 - val_loss: 0.5401 - val_acc: 0.7480\n",
      "Epoch 255/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5754 - acc: 0.7255 - val_loss: 0.5418 - val_acc: 0.7530\n",
      "Epoch 256/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5656 - acc: 0.7345 - val_loss: 0.5424 - val_acc: 0.7590\n",
      "Epoch 257/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5773 - acc: 0.7340 - val_loss: 0.5410 - val_acc: 0.7440\n",
      "Epoch 258/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5752 - acc: 0.7380 - val_loss: 0.5370 - val_acc: 0.7520\n",
      "Epoch 259/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5733 - acc: 0.7350 - val_loss: 0.5448 - val_acc: 0.7670\n",
      "Epoch 260/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5707 - acc: 0.7440 - val_loss: 0.5367 - val_acc: 0.7420\n",
      "Epoch 261/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5740 - acc: 0.7360 - val_loss: 0.5346 - val_acc: 0.7600\n",
      "Epoch 262/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5787 - acc: 0.7370 - val_loss: 0.5382 - val_acc: 0.7610\n",
      "Epoch 263/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5722 - acc: 0.7445 - val_loss: 0.5367 - val_acc: 0.7630\n",
      "Epoch 264/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5728 - acc: 0.7355 - val_loss: 0.5374 - val_acc: 0.7550\n",
      "Epoch 265/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5694 - acc: 0.7415 - val_loss: 0.5362 - val_acc: 0.7540\n",
      "Epoch 266/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5690 - acc: 0.7450 - val_loss: 0.5334 - val_acc: 0.7550\n",
      "Epoch 267/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.5670 - acc: 0.7425 - val_loss: 0.5337 - val_acc: 0.7600\n",
      "Epoch 268/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5715 - acc: 0.7350 - val_loss: 0.5371 - val_acc: 0.7610\n",
      "Epoch 269/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5695 - acc: 0.7385 - val_loss: 0.5316 - val_acc: 0.7560\n",
      "Epoch 270/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5663 - acc: 0.7405 - val_loss: 0.5337 - val_acc: 0.7600\n",
      "Epoch 271/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5685 - acc: 0.7395 - val_loss: 0.5332 - val_acc: 0.7500\n",
      "Epoch 272/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5626 - acc: 0.7460 - val_loss: 0.5314 - val_acc: 0.7600\n",
      "Epoch 273/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5587 - acc: 0.7490 - val_loss: 0.5356 - val_acc: 0.7550\n",
      "Epoch 274/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5687 - acc: 0.7420 - val_loss: 0.5286 - val_acc: 0.7580\n",
      "Epoch 275/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5640 - acc: 0.7355 - val_loss: 0.5289 - val_acc: 0.7590\n",
      "Epoch 276/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5782 - acc: 0.7390 - val_loss: 0.5352 - val_acc: 0.7650\n",
      "Epoch 277/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5617 - acc: 0.7380 - val_loss: 0.5363 - val_acc: 0.7660\n",
      "Epoch 278/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5687 - acc: 0.7405 - val_loss: 0.5302 - val_acc: 0.7590\n",
      "Epoch 279/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5623 - acc: 0.7435 - val_loss: 0.5298 - val_acc: 0.7570\n",
      "Epoch 280/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5624 - acc: 0.7335 - val_loss: 0.5303 - val_acc: 0.7650\n",
      "Epoch 281/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5611 - acc: 0.7400 - val_loss: 0.5269 - val_acc: 0.7590\n",
      "Epoch 282/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5633 - acc: 0.7460 - val_loss: 0.5282 - val_acc: 0.7620\n",
      "Epoch 283/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5343 - acc: 0.744 - 0s 28us/step - loss: 0.5644 - acc: 0.7415 - val_loss: 0.5279 - val_acc: 0.7630\n",
      "Epoch 284/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5675 - acc: 0.7405 - val_loss: 0.5317 - val_acc: 0.7660\n",
      "Epoch 285/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5666 - acc: 0.7380 - val_loss: 0.5331 - val_acc: 0.7520\n",
      "Epoch 286/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5711 - acc: 0.7415 - val_loss: 0.5266 - val_acc: 0.7610\n",
      "Epoch 287/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5649 - acc: 0.7390 - val_loss: 0.5301 - val_acc: 0.7620\n",
      "Epoch 288/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5525 - acc: 0.7540 - val_loss: 0.5235 - val_acc: 0.7600\n",
      "Epoch 289/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5693 - acc: 0.7390 - val_loss: 0.5247 - val_acc: 0.7590\n",
      "Epoch 290/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5724 - acc: 0.7300 - val_loss: 0.5255 - val_acc: 0.7610\n",
      "Epoch 291/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5664 - acc: 0.7435 - val_loss: 0.5283 - val_acc: 0.7600\n",
      "Epoch 292/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5655 - acc: 0.7460 - val_loss: 0.5260 - val_acc: 0.7560\n",
      "Epoch 293/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5631 - acc: 0.7415 - val_loss: 0.5246 - val_acc: 0.7620\n",
      "Epoch 294/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5593 - acc: 0.7470 - val_loss: 0.5305 - val_acc: 0.7580\n",
      "Epoch 295/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5684 - acc: 0.7385 - val_loss: 0.5261 - val_acc: 0.7570\n",
      "Epoch 296/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5549 - acc: 0.7515 - val_loss: 0.5262 - val_acc: 0.7600\n",
      "Epoch 297/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5579 - acc: 0.7520 - val_loss: 0.5261 - val_acc: 0.7640\n",
      "Epoch 298/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5660 - acc: 0.7390 - val_loss: 0.5261 - val_acc: 0.7650\n",
      "Epoch 299/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5569 - acc: 0.7525 - val_loss: 0.5246 - val_acc: 0.7600\n",
      "Epoch 300/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5571 - acc: 0.7520 - val_loss: 0.5233 - val_acc: 0.7600\n",
      "Epoch 301/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5639 - acc: 0.7370 - val_loss: 0.5259 - val_acc: 0.7590\n",
      "Epoch 302/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5583 - acc: 0.7445 - val_loss: 0.5231 - val_acc: 0.7590\n",
      "Epoch 303/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5607 - acc: 0.7405 - val_loss: 0.5250 - val_acc: 0.7600\n",
      "Epoch 304/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5671 - acc: 0.7460 - val_loss: 0.5240 - val_acc: 0.7660\n",
      "Epoch 305/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5588 - acc: 0.7515 - val_loss: 0.5226 - val_acc: 0.7610\n",
      "Epoch 306/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5574 - acc: 0.7530 - val_loss: 0.5216 - val_acc: 0.7620\n",
      "Epoch 307/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5560 - acc: 0.7435 - val_loss: 0.5223 - val_acc: 0.7650\n",
      "Epoch 308/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5602 - acc: 0.7370 - val_loss: 0.5221 - val_acc: 0.7660\n",
      "Epoch 309/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5634 - acc: 0.7445 - val_loss: 0.5223 - val_acc: 0.7620\n",
      "Epoch 310/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5602 - acc: 0.7510 - val_loss: 0.5207 - val_acc: 0.7630\n",
      "Epoch 311/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5600 - acc: 0.7380 - val_loss: 0.5241 - val_acc: 0.7620\n",
      "Epoch 312/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5625 - acc: 0.7380 - val_loss: 0.5207 - val_acc: 0.7630\n",
      "Epoch 313/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.5590 - acc: 0.7465 - val_loss: 0.5218 - val_acc: 0.7600\n",
      "Epoch 314/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5560 - acc: 0.7470 - val_loss: 0.5239 - val_acc: 0.7700\n",
      "Epoch 315/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5545 - acc: 0.7530 - val_loss: 0.5176 - val_acc: 0.7610\n",
      "Epoch 316/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5554 - acc: 0.7485 - val_loss: 0.5220 - val_acc: 0.7620\n",
      "Epoch 317/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5587 - acc: 0.7530 - val_loss: 0.5243 - val_acc: 0.7650\n",
      "Epoch 318/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5630 - acc: 0.7435 - val_loss: 0.5194 - val_acc: 0.7640\n",
      "Epoch 319/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5512 - acc: 0.7510 - val_loss: 0.5184 - val_acc: 0.7630\n",
      "Epoch 320/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5485 - acc: 0.7530 - val_loss: 0.5210 - val_acc: 0.7650\n",
      "Epoch 321/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5519 - acc: 0.7540 - val_loss: 0.5203 - val_acc: 0.7620\n",
      "Epoch 322/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5565 - acc: 0.7505 - val_loss: 0.5166 - val_acc: 0.7620\n",
      "Epoch 323/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5587 - acc: 0.7575 - val_loss: 0.5215 - val_acc: 0.7710\n",
      "Epoch 324/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5629 - acc: 0.7315 - val_loss: 0.5160 - val_acc: 0.7690\n",
      "Epoch 325/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5551 - acc: 0.7450 - val_loss: 0.5175 - val_acc: 0.7710\n",
      "Epoch 326/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5634 - acc: 0.7420 - val_loss: 0.5162 - val_acc: 0.7630\n",
      "Epoch 327/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5611 - acc: 0.7425 - val_loss: 0.5172 - val_acc: 0.7710\n",
      "Epoch 328/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5557 - acc: 0.7485 - val_loss: 0.5197 - val_acc: 0.7620\n",
      "Epoch 329/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5483 - acc: 0.7515 - val_loss: 0.5182 - val_acc: 0.7650\n",
      "Epoch 330/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5537 - acc: 0.7485 - val_loss: 0.5150 - val_acc: 0.7630\n",
      "Epoch 331/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5533 - acc: 0.7575 - val_loss: 0.5188 - val_acc: 0.7620\n",
      "Epoch 332/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5601 - acc: 0.7490 - val_loss: 0.5197 - val_acc: 0.7710\n",
      "Epoch 333/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5526 - acc: 0.7535 - val_loss: 0.5156 - val_acc: 0.7660\n",
      "Epoch 334/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5592 - acc: 0.7425 - val_loss: 0.5155 - val_acc: 0.7650\n",
      "Epoch 335/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5464 - acc: 0.7530 - val_loss: 0.5162 - val_acc: 0.7710\n",
      "Epoch 336/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5649 - acc: 0.7485 - val_loss: 0.5153 - val_acc: 0.7650\n",
      "Epoch 337/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5501 - acc: 0.7515 - val_loss: 0.5167 - val_acc: 0.7630\n",
      "Epoch 338/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5544 - acc: 0.7460 - val_loss: 0.5134 - val_acc: 0.7700\n",
      "Epoch 339/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5577 - acc: 0.7445 - val_loss: 0.5172 - val_acc: 0.7700\n",
      "Epoch 340/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5499 - acc: 0.7535 - val_loss: 0.5134 - val_acc: 0.7650\n",
      "Epoch 341/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5530 - acc: 0.7550 - val_loss: 0.5175 - val_acc: 0.7630\n",
      "Epoch 342/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5525 - acc: 0.7450 - val_loss: 0.5129 - val_acc: 0.7650\n",
      "Epoch 343/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5519 - acc: 0.7565 - val_loss: 0.5180 - val_acc: 0.7630\n",
      "Epoch 344/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5541 - acc: 0.7515 - val_loss: 0.5160 - val_acc: 0.7640\n",
      "Epoch 345/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5534 - acc: 0.7515 - val_loss: 0.5174 - val_acc: 0.7620\n",
      "Epoch 346/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5589 - acc: 0.7425 - val_loss: 0.5151 - val_acc: 0.7630\n",
      "Epoch 347/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5547 - acc: 0.7510 - val_loss: 0.5134 - val_acc: 0.7650\n",
      "Epoch 348/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5548 - acc: 0.7575 - val_loss: 0.5134 - val_acc: 0.7640\n",
      "Epoch 349/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5476 - acc: 0.7530 - val_loss: 0.5163 - val_acc: 0.7650\n",
      "Epoch 350/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5505 - acc: 0.7480 - val_loss: 0.5137 - val_acc: 0.7690\n",
      "Epoch 351/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5495 - acc: 0.7465 - val_loss: 0.5157 - val_acc: 0.7630\n",
      "Epoch 352/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5483 - acc: 0.7485 - val_loss: 0.5108 - val_acc: 0.7670\n",
      "Epoch 353/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5553 - acc: 0.7475 - val_loss: 0.5155 - val_acc: 0.7680\n",
      "Epoch 354/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5518 - acc: 0.7555 - val_loss: 0.5111 - val_acc: 0.7660\n",
      "Epoch 355/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5480 - acc: 0.7535 - val_loss: 0.5106 - val_acc: 0.7710\n",
      "Epoch 356/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5556 - acc: 0.7445 - val_loss: 0.5140 - val_acc: 0.7680\n",
      "Epoch 357/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5495 - acc: 0.7625 - val_loss: 0.5110 - val_acc: 0.7710\n",
      "Epoch 358/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5464 - acc: 0.7535 - val_loss: 0.5092 - val_acc: 0.7690\n",
      "Epoch 359/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5475 - acc: 0.7530 - val_loss: 0.5112 - val_acc: 0.7720\n",
      "Epoch 360/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5487 - acc: 0.7495 - val_loss: 0.5110 - val_acc: 0.7640\n",
      "Epoch 361/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5603 - acc: 0.7530 - val_loss: 0.5101 - val_acc: 0.7660\n",
      "Epoch 362/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5453 - acc: 0.7600 - val_loss: 0.5088 - val_acc: 0.7650\n",
      "Epoch 363/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5504 - acc: 0.7590 - val_loss: 0.5109 - val_acc: 0.7700\n",
      "Epoch 364/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5483 - acc: 0.7545 - val_loss: 0.5103 - val_acc: 0.7740\n",
      "Epoch 365/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5509 - acc: 0.7545 - val_loss: 0.5089 - val_acc: 0.7700\n",
      "Epoch 366/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5531 - acc: 0.7535 - val_loss: 0.5092 - val_acc: 0.7650\n",
      "Epoch 367/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5417 - acc: 0.7585 - val_loss: 0.5112 - val_acc: 0.7720\n",
      "Epoch 368/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5543 - acc: 0.7495 - val_loss: 0.5132 - val_acc: 0.7690\n",
      "Epoch 369/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5512 - acc: 0.7570 - val_loss: 0.5100 - val_acc: 0.7650\n",
      "Epoch 370/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5431 - acc: 0.7540 - val_loss: 0.5080 - val_acc: 0.7740\n",
      "Epoch 371/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5492 - acc: 0.7485 - val_loss: 0.5103 - val_acc: 0.7670\n",
      "Epoch 372/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5504 - acc: 0.7610 - val_loss: 0.5103 - val_acc: 0.7800\n",
      "Epoch 373/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5495 - acc: 0.7565 - val_loss: 0.5093 - val_acc: 0.7690\n",
      "Epoch 374/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5432 - acc: 0.7550 - val_loss: 0.5078 - val_acc: 0.7720\n",
      "Epoch 375/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5494 - acc: 0.7560 - val_loss: 0.5107 - val_acc: 0.7730\n",
      "Epoch 376/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5503 - acc: 0.7565 - val_loss: 0.5082 - val_acc: 0.7690\n",
      "Epoch 377/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5492 - acc: 0.7545 - val_loss: 0.5092 - val_acc: 0.7740\n",
      "Epoch 378/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5460 - acc: 0.7590 - val_loss: 0.5054 - val_acc: 0.7710\n",
      "Epoch 379/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5444 - acc: 0.7545 - val_loss: 0.5075 - val_acc: 0.7730\n",
      "Epoch 380/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5471 - acc: 0.7550 - val_loss: 0.5133 - val_acc: 0.7670\n",
      "Epoch 381/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5422 - acc: 0.7605 - val_loss: 0.5092 - val_acc: 0.7720\n",
      "Epoch 382/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5469 - acc: 0.7525 - val_loss: 0.5077 - val_acc: 0.7710\n",
      "Epoch 383/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5399 - acc: 0.7500 - val_loss: 0.5096 - val_acc: 0.7740\n",
      "Epoch 384/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5486 - acc: 0.7555 - val_loss: 0.5099 - val_acc: 0.7680\n",
      "Epoch 385/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5516 - acc: 0.7535 - val_loss: 0.5062 - val_acc: 0.7680\n",
      "Epoch 386/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5497 - acc: 0.7585 - val_loss: 0.5079 - val_acc: 0.7720\n",
      "Epoch 387/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5447 - acc: 0.7485 - val_loss: 0.5069 - val_acc: 0.7750\n",
      "Epoch 388/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5464 - acc: 0.7640 - val_loss: 0.5083 - val_acc: 0.7700\n",
      "Epoch 389/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5475 - acc: 0.7600 - val_loss: 0.5045 - val_acc: 0.7730\n",
      "Epoch 390/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5480 - acc: 0.7590 - val_loss: 0.5080 - val_acc: 0.7680\n",
      "Epoch 391/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5444 - acc: 0.7575 - val_loss: 0.5106 - val_acc: 0.7690\n",
      "Epoch 392/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5479 - acc: 0.7535 - val_loss: 0.5062 - val_acc: 0.7740\n",
      "Epoch 393/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5513 - acc: 0.7455 - val_loss: 0.5104 - val_acc: 0.7690\n",
      "Epoch 394/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5482 - acc: 0.7590 - val_loss: 0.5039 - val_acc: 0.7730\n",
      "Epoch 395/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5443 - acc: 0.7515 - val_loss: 0.5086 - val_acc: 0.7720\n",
      "Epoch 396/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5399 - acc: 0.7525 - val_loss: 0.5072 - val_acc: 0.7700\n",
      "Epoch 397/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5423 - acc: 0.7555 - val_loss: 0.5066 - val_acc: 0.7710\n",
      "Epoch 398/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5466 - acc: 0.7570 - val_loss: 0.5073 - val_acc: 0.7740\n",
      "Epoch 399/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5477 - acc: 0.7480 - val_loss: 0.5069 - val_acc: 0.7730\n",
      "Epoch 400/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5442 - acc: 0.7560 - val_loss: 0.5043 - val_acc: 0.7700\n",
      "Epoch 401/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5428 - acc: 0.7470 - val_loss: 0.5082 - val_acc: 0.7700\n",
      "Epoch 402/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5535 - acc: 0.7510 - val_loss: 0.5035 - val_acc: 0.7720\n",
      "Epoch 403/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5390 - acc: 0.7580 - val_loss: 0.5053 - val_acc: 0.7720\n",
      "Epoch 404/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5449 - acc: 0.7575 - val_loss: 0.5060 - val_acc: 0.7710\n",
      "Epoch 405/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5430 - acc: 0.7595 - val_loss: 0.5059 - val_acc: 0.7740\n",
      "Epoch 406/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5481 - acc: 0.7590 - val_loss: 0.5073 - val_acc: 0.7710\n",
      "Epoch 407/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5445 - acc: 0.7580 - val_loss: 0.5091 - val_acc: 0.7710\n",
      "Epoch 408/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5427 - acc: 0.7480 - val_loss: 0.5078 - val_acc: 0.7720\n",
      "Epoch 409/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5511 - acc: 0.7600 - val_loss: 0.5061 - val_acc: 0.7720\n",
      "Epoch 410/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5412 - acc: 0.7635 - val_loss: 0.5039 - val_acc: 0.7720\n",
      "Epoch 411/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5425 - acc: 0.7570 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 412/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5524 - acc: 0.7615 - val_loss: 0.5038 - val_acc: 0.7750\n",
      "Epoch 413/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5338 - acc: 0.7615 - val_loss: 0.5071 - val_acc: 0.7720\n",
      "Epoch 414/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5552 - acc: 0.7520 - val_loss: 0.5038 - val_acc: 0.7730\n",
      "Epoch 415/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5426 - acc: 0.7615 - val_loss: 0.5048 - val_acc: 0.7730\n",
      "Epoch 416/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5408 - acc: 0.7540 - val_loss: 0.5023 - val_acc: 0.7750\n",
      "Epoch 417/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5447 - acc: 0.7615 - val_loss: 0.5112 - val_acc: 0.7630\n",
      "Epoch 418/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5424 - acc: 0.7585 - val_loss: 0.5062 - val_acc: 0.7710\n",
      "Epoch 419/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5390 - acc: 0.7605 - val_loss: 0.5052 - val_acc: 0.7720\n",
      "Epoch 420/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5467 - acc: 0.7590 - val_loss: 0.5050 - val_acc: 0.7710\n",
      "Epoch 421/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5462 - acc: 0.7580 - val_loss: 0.5113 - val_acc: 0.7690\n",
      "Epoch 422/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5396 - acc: 0.7540 - val_loss: 0.5038 - val_acc: 0.7720\n",
      "Epoch 423/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5482 - acc: 0.7575 - val_loss: 0.5024 - val_acc: 0.7710\n",
      "Epoch 424/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5422 - acc: 0.760 - 0s 30us/step - loss: 0.5454 - acc: 0.7450 - val_loss: 0.5020 - val_acc: 0.7750\n",
      "Epoch 425/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5431 - acc: 0.7495 - val_loss: 0.5045 - val_acc: 0.7730\n",
      "Epoch 426/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5410 - acc: 0.7560 - val_loss: 0.5038 - val_acc: 0.7710\n",
      "Epoch 427/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5378 - acc: 0.7630 - val_loss: 0.5051 - val_acc: 0.7730\n",
      "Epoch 428/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5419 - acc: 0.7485 - val_loss: 0.5008 - val_acc: 0.7700\n",
      "Epoch 429/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5370 - acc: 0.7570 - val_loss: 0.5054 - val_acc: 0.7710\n",
      "Epoch 430/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5381 - acc: 0.7580 - val_loss: 0.5060 - val_acc: 0.7720\n",
      "Epoch 431/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5433 - acc: 0.7560 - val_loss: 0.5023 - val_acc: 0.7740\n",
      "Epoch 432/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5397 - acc: 0.7590 - val_loss: 0.5046 - val_acc: 0.7730\n",
      "Epoch 433/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5400 - acc: 0.7535 - val_loss: 0.5023 - val_acc: 0.7740\n",
      "Epoch 434/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5369 - acc: 0.7620 - val_loss: 0.5026 - val_acc: 0.7720\n",
      "Epoch 435/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5409 - acc: 0.7555 - val_loss: 0.4997 - val_acc: 0.7770\n",
      "Epoch 436/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5341 - acc: 0.7605 - val_loss: 0.5055 - val_acc: 0.7740\n",
      "Epoch 437/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5458 - acc: 0.7530 - val_loss: 0.5019 - val_acc: 0.7770\n",
      "Epoch 438/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5452 - acc: 0.7535 - val_loss: 0.5065 - val_acc: 0.7710\n",
      "Epoch 439/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5417 - acc: 0.7610 - val_loss: 0.5052 - val_acc: 0.7700\n",
      "Epoch 440/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5446 - acc: 0.7540 - val_loss: 0.5028 - val_acc: 0.7720\n",
      "Epoch 441/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5416 - acc: 0.7600 - val_loss: 0.5041 - val_acc: 0.7730\n",
      "Epoch 442/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5386 - acc: 0.7585 - val_loss: 0.5053 - val_acc: 0.7700\n",
      "Epoch 443/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5400 - acc: 0.7540 - val_loss: 0.5021 - val_acc: 0.7710\n",
      "Epoch 444/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5492 - acc: 0.7485 - val_loss: 0.5033 - val_acc: 0.7730\n",
      "Epoch 445/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5493 - acc: 0.7525 - val_loss: 0.5048 - val_acc: 0.7710\n",
      "Epoch 446/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5395 - acc: 0.7630 - val_loss: 0.5022 - val_acc: 0.7740\n",
      "Epoch 447/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5376 - acc: 0.7625 - val_loss: 0.5017 - val_acc: 0.7710\n",
      "Epoch 448/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5472 - acc: 0.7520 - val_loss: 0.5030 - val_acc: 0.7740\n",
      "Epoch 449/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5432 - acc: 0.7605 - val_loss: 0.5027 - val_acc: 0.7760\n",
      "Epoch 450/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5341 - acc: 0.7605 - val_loss: 0.5055 - val_acc: 0.7680\n",
      "Epoch 451/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5380 - acc: 0.7590 - val_loss: 0.5022 - val_acc: 0.7720\n",
      "Epoch 452/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5467 - acc: 0.7455 - val_loss: 0.5042 - val_acc: 0.7740\n",
      "Epoch 453/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5401 - acc: 0.7590 - val_loss: 0.4992 - val_acc: 0.7730\n",
      "Epoch 454/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5431 - acc: 0.7580 - val_loss: 0.5009 - val_acc: 0.7720\n",
      "Epoch 455/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5347 - acc: 0.7655 - val_loss: 0.5003 - val_acc: 0.7750\n",
      "Epoch 456/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5355 - acc: 0.7645 - val_loss: 0.5050 - val_acc: 0.7710\n",
      "Epoch 457/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5387 - acc: 0.7560 - val_loss: 0.4991 - val_acc: 0.7740\n",
      "Epoch 458/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5415 - acc: 0.7575 - val_loss: 0.5022 - val_acc: 0.7720\n",
      "Epoch 459/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5385 - acc: 0.7650 - val_loss: 0.4999 - val_acc: 0.7730\n",
      "Epoch 460/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5327 - acc: 0.7605 - val_loss: 0.5014 - val_acc: 0.7770\n",
      "Epoch 461/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5293 - acc: 0.7600 - val_loss: 0.5015 - val_acc: 0.7760\n",
      "Epoch 462/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5361 - acc: 0.7580 - val_loss: 0.5012 - val_acc: 0.7700\n",
      "Epoch 463/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5388 - acc: 0.7560 - val_loss: 0.4987 - val_acc: 0.7760\n",
      "Epoch 464/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5399 - acc: 0.7595 - val_loss: 0.5051 - val_acc: 0.7720\n",
      "Epoch 465/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5383 - acc: 0.7575 - val_loss: 0.4992 - val_acc: 0.7730\n",
      "Epoch 466/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5397 - acc: 0.7655 - val_loss: 0.5010 - val_acc: 0.7720\n",
      "Epoch 467/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5341 - acc: 0.7590 - val_loss: 0.5037 - val_acc: 0.7650\n",
      "Epoch 468/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5414 - acc: 0.7575 - val_loss: 0.4999 - val_acc: 0.7740\n",
      "Epoch 469/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5362 - acc: 0.7610 - val_loss: 0.5004 - val_acc: 0.7750\n",
      "Epoch 470/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5406 - acc: 0.7560 - val_loss: 0.5035 - val_acc: 0.7720\n",
      "Epoch 471/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5432 - acc: 0.7535 - val_loss: 0.5006 - val_acc: 0.7720\n",
      "Epoch 472/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5350 - acc: 0.7655 - val_loss: 0.5017 - val_acc: 0.7740\n",
      "Epoch 473/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5375 - acc: 0.7595 - val_loss: 0.5030 - val_acc: 0.7780\n",
      "Epoch 474/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5397 - acc: 0.7585 - val_loss: 0.5000 - val_acc: 0.7740\n",
      "Epoch 475/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5348 - acc: 0.7590 - val_loss: 0.5023 - val_acc: 0.7700\n",
      "Epoch 476/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5435 - acc: 0.7565 - val_loss: 0.5096 - val_acc: 0.7790\n",
      "Epoch 477/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5400 - acc: 0.7550 - val_loss: 0.4982 - val_acc: 0.7740\n",
      "Epoch 478/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5343 - acc: 0.7655 - val_loss: 0.5002 - val_acc: 0.7730\n",
      "Epoch 479/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5377 - acc: 0.7605 - val_loss: 0.5005 - val_acc: 0.7730\n",
      "Epoch 480/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5327 - acc: 0.7575 - val_loss: 0.5005 - val_acc: 0.7760\n",
      "Epoch 481/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5354 - acc: 0.7605 - val_loss: 0.4991 - val_acc: 0.7740\n",
      "Epoch 482/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5300 - acc: 0.7590 - val_loss: 0.4974 - val_acc: 0.7740\n",
      "Epoch 483/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5372 - acc: 0.7565 - val_loss: 0.4970 - val_acc: 0.7710\n",
      "Epoch 484/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5425 - acc: 0.7570 - val_loss: 0.4989 - val_acc: 0.7720\n",
      "Epoch 485/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5303 - acc: 0.7725 - val_loss: 0.4970 - val_acc: 0.7730\n",
      "Epoch 486/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5463 - acc: 0.7480 - val_loss: 0.4979 - val_acc: 0.7720\n",
      "Epoch 487/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5416 - acc: 0.7620 - val_loss: 0.4986 - val_acc: 0.7750\n",
      "Epoch 488/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5336 - acc: 0.7645 - val_loss: 0.4964 - val_acc: 0.7720\n",
      "Epoch 489/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5320 - acc: 0.7505 - val_loss: 0.4984 - val_acc: 0.7750\n",
      "Epoch 490/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5427 - acc: 0.7520 - val_loss: 0.4975 - val_acc: 0.7720\n",
      "Epoch 491/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5375 - acc: 0.7515 - val_loss: 0.4968 - val_acc: 0.7760\n",
      "Epoch 492/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5323 - acc: 0.7615 - val_loss: 0.5006 - val_acc: 0.7690\n",
      "Epoch 493/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5373 - acc: 0.7545 - val_loss: 0.4980 - val_acc: 0.7720\n",
      "Epoch 494/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5366 - acc: 0.7590 - val_loss: 0.4981 - val_acc: 0.7750\n",
      "Epoch 495/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5398 - acc: 0.7565 - val_loss: 0.4966 - val_acc: 0.7720\n",
      "Epoch 496/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5353 - acc: 0.7650 - val_loss: 0.4997 - val_acc: 0.7750\n",
      "Epoch 497/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5300 - acc: 0.7640 - val_loss: 0.5023 - val_acc: 0.7760\n",
      "Epoch 498/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5446 - acc: 0.7530 - val_loss: 0.4981 - val_acc: 0.7750\n",
      "Epoch 499/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5392 - acc: 0.7620 - val_loss: 0.5015 - val_acc: 0.7770\n",
      "Epoch 500/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5405 - acc: 0.7555 - val_loss: 0.5017 - val_acc: 0.7750\n",
      "Epoch 501/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5361 - acc: 0.7600 - val_loss: 0.4963 - val_acc: 0.7760\n",
      "Epoch 502/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5405 - acc: 0.7605 - val_loss: 0.4980 - val_acc: 0.7730\n",
      "Epoch 503/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5362 - acc: 0.7615 - val_loss: 0.4996 - val_acc: 0.7770\n",
      "Epoch 504/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5429 - acc: 0.7515 - val_loss: 0.4984 - val_acc: 0.7770\n",
      "Epoch 505/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5341 - acc: 0.7635 - val_loss: 0.4978 - val_acc: 0.7750\n",
      "Epoch 506/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5372 - acc: 0.7605 - val_loss: 0.4972 - val_acc: 0.7720\n",
      "Epoch 507/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5363 - acc: 0.7605 - val_loss: 0.5013 - val_acc: 0.7780\n",
      "Epoch 508/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5353 - acc: 0.7555 - val_loss: 0.5031 - val_acc: 0.7760\n",
      "Epoch 509/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5374 - acc: 0.7625 - val_loss: 0.4965 - val_acc: 0.7760\n",
      "Epoch 510/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5412 - acc: 0.7580 - val_loss: 0.5010 - val_acc: 0.7730\n",
      "Epoch 511/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5330 - acc: 0.7585 - val_loss: 0.4994 - val_acc: 0.7720\n",
      "Epoch 512/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5363 - acc: 0.7545 - val_loss: 0.4977 - val_acc: 0.7730\n",
      "Epoch 513/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5402 - acc: 0.7635 - val_loss: 0.5022 - val_acc: 0.7770\n",
      "Epoch 514/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5377 - acc: 0.7655 - val_loss: 0.4970 - val_acc: 0.7770\n",
      "Epoch 515/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5366 - acc: 0.7575 - val_loss: 0.4985 - val_acc: 0.7760\n",
      "Epoch 516/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.5355 - acc: 0.7605 - val_loss: 0.4979 - val_acc: 0.7730\n",
      "Epoch 517/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5306 - acc: 0.7635 - val_loss: 0.4978 - val_acc: 0.7740\n",
      "Epoch 518/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5364 - acc: 0.7585 - val_loss: 0.4980 - val_acc: 0.7720\n",
      "Epoch 519/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5300 - acc: 0.7630 - val_loss: 0.4974 - val_acc: 0.7760\n",
      "Epoch 520/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5309 - acc: 0.7680 - val_loss: 0.4971 - val_acc: 0.7740\n",
      "Epoch 521/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5382 - acc: 0.7530 - val_loss: 0.4954 - val_acc: 0.7740\n",
      "Epoch 522/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5365 - acc: 0.7600 - val_loss: 0.4941 - val_acc: 0.7780\n",
      "Epoch 523/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5304 - acc: 0.7560 - val_loss: 0.4967 - val_acc: 0.7730\n",
      "Epoch 524/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5294 - acc: 0.7610 - val_loss: 0.4962 - val_acc: 0.7740\n",
      "Epoch 525/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5434 - acc: 0.7550 - val_loss: 0.4987 - val_acc: 0.7760\n",
      "Epoch 526/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5413 - acc: 0.7600 - val_loss: 0.4962 - val_acc: 0.7730\n",
      "Epoch 527/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5420 - acc: 0.7575 - val_loss: 0.4977 - val_acc: 0.7750\n",
      "Epoch 528/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5386 - acc: 0.7560 - val_loss: 0.4956 - val_acc: 0.7720\n",
      "Epoch 529/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5376 - acc: 0.7585 - val_loss: 0.4962 - val_acc: 0.7740\n",
      "Epoch 530/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5346 - acc: 0.7665 - val_loss: 0.4944 - val_acc: 0.7730\n",
      "Epoch 531/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5374 - acc: 0.7510 - val_loss: 0.4967 - val_acc: 0.7750\n",
      "Epoch 532/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5351 - acc: 0.7570 - val_loss: 0.4987 - val_acc: 0.7740\n",
      "Epoch 533/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5321 - acc: 0.7680 - val_loss: 0.4979 - val_acc: 0.7730\n",
      "Epoch 534/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5290 - acc: 0.7605 - val_loss: 0.4992 - val_acc: 0.7700\n",
      "Epoch 535/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5328 - acc: 0.7625 - val_loss: 0.4966 - val_acc: 0.7720\n",
      "Epoch 536/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5305 - acc: 0.7580 - val_loss: 0.4983 - val_acc: 0.7700\n",
      "Epoch 537/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5295 - acc: 0.7610 - val_loss: 0.4944 - val_acc: 0.7740\n",
      "Epoch 538/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5385 - acc: 0.7680 - val_loss: 0.4969 - val_acc: 0.7730\n",
      "Epoch 539/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5305 - acc: 0.7680 - val_loss: 0.4979 - val_acc: 0.7760\n",
      "Epoch 540/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5359 - acc: 0.7620 - val_loss: 0.4959 - val_acc: 0.7720\n",
      "Epoch 541/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5364 - acc: 0.7580 - val_loss: 0.4944 - val_acc: 0.7740\n",
      "Epoch 542/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5410 - acc: 0.7575 - val_loss: 0.5023 - val_acc: 0.7720\n",
      "Epoch 543/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5328 - acc: 0.7665 - val_loss: 0.4975 - val_acc: 0.7710\n",
      "Epoch 544/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5315 - acc: 0.7640 - val_loss: 0.4985 - val_acc: 0.7720\n",
      "Epoch 545/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5311 - acc: 0.7575 - val_loss: 0.4967 - val_acc: 0.7750\n",
      "Epoch 546/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5351 - acc: 0.7640 - val_loss: 0.4985 - val_acc: 0.7750\n",
      "Epoch 547/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5339 - acc: 0.7620 - val_loss: 0.4958 - val_acc: 0.7750\n",
      "Epoch 548/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5410 - acc: 0.7535 - val_loss: 0.4976 - val_acc: 0.7750\n",
      "Epoch 549/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5334 - acc: 0.7545 - val_loss: 0.4965 - val_acc: 0.7720\n",
      "Epoch 550/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5350 - acc: 0.7570 - val_loss: 0.4988 - val_acc: 0.7730\n",
      "Epoch 551/800\n",
      "2000/2000 [==============================] - 0s 52us/step - loss: 0.5323 - acc: 0.7590 - val_loss: 0.4954 - val_acc: 0.7720\n",
      "Epoch 552/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.5278 - acc: 0.7675 - val_loss: 0.4968 - val_acc: 0.7720\n",
      "Epoch 553/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.5301 - acc: 0.7630 - val_loss: 0.4955 - val_acc: 0.7730\n",
      "Epoch 554/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5338 - acc: 0.7515 - val_loss: 0.4969 - val_acc: 0.7740\n",
      "Epoch 555/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5308 - acc: 0.7600 - val_loss: 0.4998 - val_acc: 0.7700\n",
      "Epoch 556/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5371 - acc: 0.7580 - val_loss: 0.4947 - val_acc: 0.7740\n",
      "Epoch 557/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5278 - acc: 0.7730 - val_loss: 0.4979 - val_acc: 0.7740\n",
      "Epoch 558/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5449 - acc: 0.7565 - val_loss: 0.4983 - val_acc: 0.7740\n",
      "Epoch 559/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5257 - acc: 0.7620 - val_loss: 0.4955 - val_acc: 0.7730\n",
      "Epoch 560/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5317 - acc: 0.7570 - val_loss: 0.4965 - val_acc: 0.7760\n",
      "Epoch 561/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5318 - acc: 0.7600 - val_loss: 0.4971 - val_acc: 0.7730\n",
      "Epoch 562/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5351 - acc: 0.7550 - val_loss: 0.4978 - val_acc: 0.7680\n",
      "Epoch 563/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5276 - acc: 0.7610 - val_loss: 0.4954 - val_acc: 0.7750\n",
      "Epoch 564/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5239 - acc: 0.7685 - val_loss: 0.4985 - val_acc: 0.7740\n",
      "Epoch 565/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5245 - acc: 0.7685 - val_loss: 0.4943 - val_acc: 0.7710\n",
      "Epoch 566/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5334 - acc: 0.7600 - val_loss: 0.4996 - val_acc: 0.7750\n",
      "Epoch 567/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5341 - acc: 0.7570 - val_loss: 0.4944 - val_acc: 0.7730\n",
      "Epoch 568/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5278 - acc: 0.7640 - val_loss: 0.4969 - val_acc: 0.7750\n",
      "Epoch 569/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5409 - acc: 0.7540 - val_loss: 0.4978 - val_acc: 0.7740\n",
      "Epoch 570/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5314 - acc: 0.7670 - val_loss: 0.4948 - val_acc: 0.7730\n",
      "Epoch 571/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5196 - acc: 0.7655 - val_loss: 0.4946 - val_acc: 0.7780\n",
      "Epoch 572/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5397 - acc: 0.7565 - val_loss: 0.4956 - val_acc: 0.7760\n",
      "Epoch 573/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5301 - acc: 0.7645 - val_loss: 0.4957 - val_acc: 0.7720\n",
      "Epoch 574/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5313 - acc: 0.7620 - val_loss: 0.4965 - val_acc: 0.7760\n",
      "Epoch 575/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5275 - acc: 0.7640 - val_loss: 0.4942 - val_acc: 0.7760\n",
      "Epoch 576/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5308 - acc: 0.7630 - val_loss: 0.4934 - val_acc: 0.7750\n",
      "Epoch 577/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5284 - acc: 0.7680 - val_loss: 0.4939 - val_acc: 0.7740\n",
      "Epoch 578/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5348 - acc: 0.7630 - val_loss: 0.4972 - val_acc: 0.7750\n",
      "Epoch 579/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5294 - acc: 0.7670 - val_loss: 0.4935 - val_acc: 0.7740\n",
      "Epoch 580/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5358 - acc: 0.7620 - val_loss: 0.4959 - val_acc: 0.7750\n",
      "Epoch 581/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5291 - acc: 0.7660 - val_loss: 0.4960 - val_acc: 0.7730\n",
      "Epoch 582/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5252 - acc: 0.7580 - val_loss: 0.4957 - val_acc: 0.7740\n",
      "Epoch 583/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5331 - acc: 0.7620 - val_loss: 0.4984 - val_acc: 0.7730\n",
      "Epoch 584/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5274 - acc: 0.7615 - val_loss: 0.4975 - val_acc: 0.7740\n",
      "Epoch 585/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5253 - acc: 0.7650 - val_loss: 0.4952 - val_acc: 0.7730\n",
      "Epoch 586/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5371 - acc: 0.7630 - val_loss: 0.4964 - val_acc: 0.7710\n",
      "Epoch 587/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5343 - acc: 0.7545 - val_loss: 0.4954 - val_acc: 0.7700\n",
      "Epoch 588/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5349 - acc: 0.7660 - val_loss: 0.4952 - val_acc: 0.7740\n",
      "Epoch 589/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5318 - acc: 0.744 - 0s 28us/step - loss: 0.5312 - acc: 0.7580 - val_loss: 0.4941 - val_acc: 0.7750\n",
      "Epoch 590/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5399 - acc: 0.7540 - val_loss: 0.4956 - val_acc: 0.7730\n",
      "Epoch 591/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5273 - acc: 0.7690 - val_loss: 0.4945 - val_acc: 0.7710\n",
      "Epoch 592/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5324 - acc: 0.7680 - val_loss: 0.4949 - val_acc: 0.7740\n",
      "Epoch 593/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5296 - acc: 0.7625 - val_loss: 0.4935 - val_acc: 0.7730\n",
      "Epoch 594/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5275 - acc: 0.7705 - val_loss: 0.4937 - val_acc: 0.7720\n",
      "Epoch 595/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5339 - acc: 0.7605 - val_loss: 0.4945 - val_acc: 0.7740\n",
      "Epoch 596/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5302 - acc: 0.7605 - val_loss: 0.4967 - val_acc: 0.7720\n",
      "Epoch 597/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5333 - acc: 0.7585 - val_loss: 0.4937 - val_acc: 0.7750\n",
      "Epoch 598/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5332 - acc: 0.7630 - val_loss: 0.5000 - val_acc: 0.7690\n",
      "Epoch 599/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5268 - acc: 0.7610 - val_loss: 0.5000 - val_acc: 0.7750\n",
      "Epoch 600/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5308 - acc: 0.7630 - val_loss: 0.4978 - val_acc: 0.7750\n",
      "Epoch 601/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5347 - acc: 0.7650 - val_loss: 0.4936 - val_acc: 0.7760\n",
      "Epoch 602/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5307 - acc: 0.7585 - val_loss: 0.4984 - val_acc: 0.7740\n",
      "Epoch 603/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5254 - acc: 0.7630 - val_loss: 0.4978 - val_acc: 0.7730\n",
      "Epoch 604/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5311 - acc: 0.7555 - val_loss: 0.4963 - val_acc: 0.7750\n",
      "Epoch 605/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5230 - acc: 0.7520 - val_loss: 0.4964 - val_acc: 0.7720\n",
      "Epoch 606/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5258 - acc: 0.7630 - val_loss: 0.4927 - val_acc: 0.7740\n",
      "Epoch 607/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5275 - acc: 0.7645 - val_loss: 0.4912 - val_acc: 0.7760\n",
      "Epoch 608/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5297 - acc: 0.7605 - val_loss: 0.4972 - val_acc: 0.7740\n",
      "Epoch 609/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5318 - acc: 0.7645 - val_loss: 0.4960 - val_acc: 0.7740\n",
      "Epoch 610/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5362 - acc: 0.7560 - val_loss: 0.4940 - val_acc: 0.7740\n",
      "Epoch 611/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5301 - acc: 0.7590 - val_loss: 0.4935 - val_acc: 0.7750\n",
      "Epoch 612/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5367 - acc: 0.7580 - val_loss: 0.4943 - val_acc: 0.7750\n",
      "Epoch 613/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5258 - acc: 0.7655 - val_loss: 0.4970 - val_acc: 0.7740\n",
      "Epoch 614/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5240 - acc: 0.7645 - val_loss: 0.4930 - val_acc: 0.7770\n",
      "Epoch 615/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5290 - acc: 0.7615 - val_loss: 0.4939 - val_acc: 0.7770\n",
      "Epoch 616/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5262 - acc: 0.7600 - val_loss: 0.4950 - val_acc: 0.7750\n",
      "Epoch 617/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5373 - acc: 0.7600 - val_loss: 0.4928 - val_acc: 0.7730\n",
      "Epoch 618/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5330 - acc: 0.7610 - val_loss: 0.4941 - val_acc: 0.7780\n",
      "Epoch 619/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5385 - acc: 0.7590 - val_loss: 0.4942 - val_acc: 0.7710\n",
      "Epoch 620/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5357 - acc: 0.7655 - val_loss: 0.4949 - val_acc: 0.7730\n",
      "Epoch 621/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5275 - acc: 0.7590 - val_loss: 0.4946 - val_acc: 0.7770\n",
      "Epoch 622/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5277 - acc: 0.7655 - val_loss: 0.4964 - val_acc: 0.7730\n",
      "Epoch 623/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5267 - acc: 0.7655 - val_loss: 0.4978 - val_acc: 0.7740\n",
      "Epoch 624/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5259 - acc: 0.7610 - val_loss: 0.4940 - val_acc: 0.7750\n",
      "Epoch 625/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5326 - acc: 0.7570 - val_loss: 0.4918 - val_acc: 0.7740\n",
      "Epoch 626/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5331 - acc: 0.7590 - val_loss: 0.4972 - val_acc: 0.7750\n",
      "Epoch 627/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5318 - acc: 0.7625 - val_loss: 0.4958 - val_acc: 0.7730\n",
      "Epoch 628/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5328 - acc: 0.7620 - val_loss: 0.4944 - val_acc: 0.7730\n",
      "Epoch 629/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5288 - acc: 0.7640 - val_loss: 0.4939 - val_acc: 0.7710\n",
      "Epoch 630/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5344 - acc: 0.7575 - val_loss: 0.4949 - val_acc: 0.7740\n",
      "Epoch 631/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5329 - acc: 0.7570 - val_loss: 0.4969 - val_acc: 0.7730\n",
      "Epoch 632/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5365 - acc: 0.7590 - val_loss: 0.4956 - val_acc: 0.7710\n",
      "Epoch 633/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5320 - acc: 0.7565 - val_loss: 0.4934 - val_acc: 0.7740\n",
      "Epoch 634/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5241 - acc: 0.7610 - val_loss: 0.4942 - val_acc: 0.7740\n",
      "Epoch 635/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5330 - acc: 0.7570 - val_loss: 0.4953 - val_acc: 0.7730\n",
      "Epoch 636/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5342 - acc: 0.7690 - val_loss: 0.4955 - val_acc: 0.7730\n",
      "Epoch 637/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5300 - acc: 0.7620 - val_loss: 0.4941 - val_acc: 0.7740\n",
      "Epoch 638/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5270 - acc: 0.7630 - val_loss: 0.4947 - val_acc: 0.7750\n",
      "Epoch 639/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5282 - acc: 0.7670 - val_loss: 0.4964 - val_acc: 0.7710\n",
      "Epoch 640/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5294 - acc: 0.7650 - val_loss: 0.4932 - val_acc: 0.7730\n",
      "Epoch 641/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5294 - acc: 0.7625 - val_loss: 0.5000 - val_acc: 0.7730\n",
      "Epoch 642/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5288 - acc: 0.7615 - val_loss: 0.4947 - val_acc: 0.7750\n",
      "Epoch 643/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5312 - acc: 0.7655 - val_loss: 0.4963 - val_acc: 0.7740\n",
      "Epoch 644/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5339 - acc: 0.7560 - val_loss: 0.4966 - val_acc: 0.7750\n",
      "Epoch 645/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5299 - acc: 0.7600 - val_loss: 0.4949 - val_acc: 0.7750\n",
      "Epoch 646/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5341 - acc: 0.7610 - val_loss: 0.4937 - val_acc: 0.7720\n",
      "Epoch 647/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5330 - acc: 0.7600 - val_loss: 0.4931 - val_acc: 0.7730\n",
      "Epoch 648/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5219 - acc: 0.7685 - val_loss: 0.4933 - val_acc: 0.7760\n",
      "Epoch 649/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5272 - acc: 0.7675 - val_loss: 0.4947 - val_acc: 0.7740\n",
      "Epoch 650/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5299 - acc: 0.7655 - val_loss: 0.4976 - val_acc: 0.7760\n",
      "Epoch 651/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5291 - acc: 0.7535 - val_loss: 0.4944 - val_acc: 0.7730\n",
      "Epoch 652/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5285 - acc: 0.7640 - val_loss: 0.4951 - val_acc: 0.7710\n",
      "Epoch 653/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5296 - acc: 0.7585 - val_loss: 0.4940 - val_acc: 0.7740\n",
      "Epoch 654/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5294 - acc: 0.7580 - val_loss: 0.4923 - val_acc: 0.7780\n",
      "Epoch 655/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5250 - acc: 0.7640 - val_loss: 0.4925 - val_acc: 0.7760\n",
      "Epoch 656/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5226 - acc: 0.7650 - val_loss: 0.4931 - val_acc: 0.7750\n",
      "Epoch 657/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5239 - acc: 0.7650 - val_loss: 0.4934 - val_acc: 0.7710\n",
      "Epoch 658/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5315 - acc: 0.7670 - val_loss: 0.4931 - val_acc: 0.7750\n",
      "Epoch 659/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5348 - acc: 0.7585 - val_loss: 0.4957 - val_acc: 0.7740\n",
      "Epoch 660/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5331 - acc: 0.7555 - val_loss: 0.4952 - val_acc: 0.7750\n",
      "Epoch 661/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5165 - acc: 0.7705 - val_loss: 0.4965 - val_acc: 0.7670\n",
      "Epoch 662/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5301 - acc: 0.7665 - val_loss: 0.4923 - val_acc: 0.7750\n",
      "Epoch 663/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4953 - acc: 0.778 - 0s 29us/step - loss: 0.5269 - acc: 0.7625 - val_loss: 0.4949 - val_acc: 0.7750\n",
      "Epoch 664/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5250 - acc: 0.7625 - val_loss: 0.4927 - val_acc: 0.7750\n",
      "Epoch 665/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5322 - acc: 0.7640 - val_loss: 0.4954 - val_acc: 0.7720\n",
      "Epoch 666/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5262 - acc: 0.7660 - val_loss: 0.4942 - val_acc: 0.7730\n",
      "Epoch 667/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5233 - acc: 0.7640 - val_loss: 0.4932 - val_acc: 0.7720\n",
      "Epoch 668/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5228 - acc: 0.7685 - val_loss: 0.4945 - val_acc: 0.7740\n",
      "Epoch 669/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5325 - acc: 0.7660 - val_loss: 0.4948 - val_acc: 0.7720\n",
      "Epoch 670/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.5314 - acc: 0.7605 - val_loss: 0.4958 - val_acc: 0.7730\n",
      "Epoch 671/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5237 - acc: 0.7690 - val_loss: 0.4922 - val_acc: 0.7730\n",
      "Epoch 672/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5319 - acc: 0.7620 - val_loss: 0.4951 - val_acc: 0.7720\n",
      "Epoch 673/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5237 - acc: 0.7720 - val_loss: 0.4931 - val_acc: 0.7750\n",
      "Epoch 674/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5316 - acc: 0.7600 - val_loss: 0.4939 - val_acc: 0.7740\n",
      "Epoch 675/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5236 - acc: 0.7745 - val_loss: 0.4947 - val_acc: 0.7730\n",
      "Epoch 676/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5262 - acc: 0.7605 - val_loss: 0.4922 - val_acc: 0.7750\n",
      "Epoch 677/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5285 - acc: 0.7555 - val_loss: 0.4949 - val_acc: 0.7740\n",
      "Epoch 678/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5299 - acc: 0.7615 - val_loss: 0.4944 - val_acc: 0.7720\n",
      "Epoch 679/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5226 - acc: 0.7620 - val_loss: 0.4906 - val_acc: 0.7730\n",
      "Epoch 680/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5273 - acc: 0.7545 - val_loss: 0.4937 - val_acc: 0.7720\n",
      "Epoch 681/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5220 - acc: 0.7765 - val_loss: 0.4942 - val_acc: 0.7730\n",
      "Epoch 682/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5280 - acc: 0.7675 - val_loss: 0.4911 - val_acc: 0.7740\n",
      "Epoch 683/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5335 - acc: 0.7535 - val_loss: 0.4922 - val_acc: 0.7730\n",
      "Epoch 684/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5214 - acc: 0.7620 - val_loss: 0.4920 - val_acc: 0.7750\n",
      "Epoch 685/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5299 - acc: 0.7565 - val_loss: 0.4950 - val_acc: 0.7720\n",
      "Epoch 686/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5257 - acc: 0.7620 - val_loss: 0.4921 - val_acc: 0.7730\n",
      "Epoch 687/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5289 - acc: 0.7580 - val_loss: 0.4927 - val_acc: 0.7740\n",
      "Epoch 688/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5326 - acc: 0.7640 - val_loss: 0.4918 - val_acc: 0.7750\n",
      "Epoch 689/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5367 - acc: 0.7585 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 690/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5269 - acc: 0.7675 - val_loss: 0.4915 - val_acc: 0.7730\n",
      "Epoch 691/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5309 - acc: 0.7615 - val_loss: 0.4940 - val_acc: 0.7710\n",
      "Epoch 692/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5282 - acc: 0.7660 - val_loss: 0.4930 - val_acc: 0.7730\n",
      "Epoch 693/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5313 - acc: 0.7495 - val_loss: 0.4926 - val_acc: 0.7720\n",
      "Epoch 694/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5287 - acc: 0.7595 - val_loss: 0.4912 - val_acc: 0.7750\n",
      "Epoch 695/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5194 - acc: 0.7665 - val_loss: 0.4943 - val_acc: 0.7720\n",
      "Epoch 696/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5272 - acc: 0.7650 - val_loss: 0.4936 - val_acc: 0.7750\n",
      "Epoch 697/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5278 - acc: 0.7610 - val_loss: 0.4964 - val_acc: 0.7740\n",
      "Epoch 698/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5261 - acc: 0.7650 - val_loss: 0.4957 - val_acc: 0.7750\n",
      "Epoch 699/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5257 - acc: 0.7615 - val_loss: 0.4929 - val_acc: 0.7740\n",
      "Epoch 700/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5252 - acc: 0.7595 - val_loss: 0.4940 - val_acc: 0.7740\n",
      "Epoch 701/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5326 - acc: 0.7580 - val_loss: 0.4952 - val_acc: 0.7750\n",
      "Epoch 702/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5254 - acc: 0.7640 - val_loss: 0.4924 - val_acc: 0.7730\n",
      "Epoch 703/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5291 - acc: 0.7580 - val_loss: 0.4939 - val_acc: 0.7720\n",
      "Epoch 704/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5315 - acc: 0.7620 - val_loss: 0.4936 - val_acc: 0.7690\n",
      "Epoch 705/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5329 - acc: 0.7570 - val_loss: 0.4931 - val_acc: 0.7710\n",
      "Epoch 706/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5291 - acc: 0.7570 - val_loss: 0.4933 - val_acc: 0.7720\n",
      "Epoch 707/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5295 - acc: 0.7620 - val_loss: 0.4930 - val_acc: 0.7700\n",
      "Epoch 708/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5304 - acc: 0.7625 - val_loss: 0.4947 - val_acc: 0.7730\n",
      "Epoch 709/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5259 - acc: 0.7665 - val_loss: 0.4932 - val_acc: 0.7750\n",
      "Epoch 710/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5278 - acc: 0.7565 - val_loss: 0.4962 - val_acc: 0.7730\n",
      "Epoch 711/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5265 - acc: 0.7680 - val_loss: 0.4931 - val_acc: 0.7740\n",
      "Epoch 712/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5271 - acc: 0.7625 - val_loss: 0.4967 - val_acc: 0.7700\n",
      "Epoch 713/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5250 - acc: 0.7590 - val_loss: 0.4923 - val_acc: 0.7750\n",
      "Epoch 714/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5306 - acc: 0.7620 - val_loss: 0.4927 - val_acc: 0.7700\n",
      "Epoch 715/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5262 - acc: 0.7620 - val_loss: 0.4986 - val_acc: 0.7690\n",
      "Epoch 716/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5296 - acc: 0.7615 - val_loss: 0.4950 - val_acc: 0.7730\n",
      "Epoch 717/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5243 - acc: 0.7640 - val_loss: 0.4930 - val_acc: 0.7750\n",
      "Epoch 718/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5313 - acc: 0.7605 - val_loss: 0.4947 - val_acc: 0.7710\n",
      "Epoch 719/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5281 - acc: 0.7605 - val_loss: 0.4950 - val_acc: 0.7740\n",
      "Epoch 720/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5272 - acc: 0.7655 - val_loss: 0.4913 - val_acc: 0.7750\n",
      "Epoch 721/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5291 - acc: 0.7630 - val_loss: 0.4985 - val_acc: 0.7780\n",
      "Epoch 722/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5315 - acc: 0.7625 - val_loss: 0.4918 - val_acc: 0.7750\n",
      "Epoch 723/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5223 - acc: 0.7685 - val_loss: 0.4940 - val_acc: 0.7750\n",
      "Epoch 724/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5258 - acc: 0.7595 - val_loss: 0.4927 - val_acc: 0.7740\n",
      "Epoch 725/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5360 - acc: 0.7560 - val_loss: 0.4957 - val_acc: 0.7760\n",
      "Epoch 726/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5257 - acc: 0.7620 - val_loss: 0.4929 - val_acc: 0.7710\n",
      "Epoch 727/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5287 - acc: 0.7625 - val_loss: 0.4961 - val_acc: 0.7720\n",
      "Epoch 728/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5299 - acc: 0.7600 - val_loss: 0.4941 - val_acc: 0.7710\n",
      "Epoch 729/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5278 - acc: 0.7640 - val_loss: 0.4963 - val_acc: 0.7730\n",
      "Epoch 730/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5256 - acc: 0.7620 - val_loss: 0.4934 - val_acc: 0.7750\n",
      "Epoch 731/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5315 - acc: 0.7635 - val_loss: 0.4934 - val_acc: 0.7740\n",
      "Epoch 732/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5216 - acc: 0.7615 - val_loss: 0.4939 - val_acc: 0.7740\n",
      "Epoch 733/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5251 - acc: 0.7625 - val_loss: 0.4922 - val_acc: 0.7710\n",
      "Epoch 734/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5238 - acc: 0.7630 - val_loss: 0.4943 - val_acc: 0.7740\n",
      "Epoch 735/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5224 - acc: 0.7635 - val_loss: 0.4945 - val_acc: 0.7750\n",
      "Epoch 736/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5284 - acc: 0.7600 - val_loss: 0.4946 - val_acc: 0.7740\n",
      "Epoch 737/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5290 - acc: 0.7700 - val_loss: 0.4925 - val_acc: 0.7720\n",
      "Epoch 738/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5259 - acc: 0.7685 - val_loss: 0.4981 - val_acc: 0.7760\n",
      "Epoch 739/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5209 - acc: 0.7660 - val_loss: 0.4920 - val_acc: 0.7720\n",
      "Epoch 740/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5328 - acc: 0.7595 - val_loss: 0.4943 - val_acc: 0.7730\n",
      "Epoch 741/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5217 - acc: 0.7660 - val_loss: 0.4950 - val_acc: 0.7780\n",
      "Epoch 742/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5254 - acc: 0.7680 - val_loss: 0.4945 - val_acc: 0.7730\n",
      "Epoch 743/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5257 - acc: 0.7645 - val_loss: 0.4936 - val_acc: 0.7720\n",
      "Epoch 744/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5216 - acc: 0.7595 - val_loss: 0.4933 - val_acc: 0.7740\n",
      "Epoch 745/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5248 - acc: 0.7615 - val_loss: 0.4960 - val_acc: 0.7720\n",
      "Epoch 746/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5292 - acc: 0.7600 - val_loss: 0.4954 - val_acc: 0.7720\n",
      "Epoch 747/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5177 - acc: 0.7615 - val_loss: 0.4956 - val_acc: 0.7730\n",
      "Epoch 748/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5276 - acc: 0.7605 - val_loss: 0.4918 - val_acc: 0.7740\n",
      "Epoch 749/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5295 - acc: 0.7620 - val_loss: 0.4925 - val_acc: 0.7720\n",
      "Epoch 750/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5258 - acc: 0.7700 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 751/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5297 - acc: 0.7610 - val_loss: 0.4943 - val_acc: 0.7730\n",
      "Epoch 752/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5180 - acc: 0.7570 - val_loss: 0.4909 - val_acc: 0.7730\n",
      "Epoch 753/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5206 - acc: 0.7685 - val_loss: 0.4980 - val_acc: 0.7760\n",
      "Epoch 754/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5227 - acc: 0.7615 - val_loss: 0.4928 - val_acc: 0.7730\n",
      "Epoch 755/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5315 - acc: 0.7615 - val_loss: 0.4954 - val_acc: 0.7720\n",
      "Epoch 756/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5268 - acc: 0.7620 - val_loss: 0.4934 - val_acc: 0.7720\n",
      "Epoch 757/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5305 - acc: 0.7615 - val_loss: 0.4953 - val_acc: 0.7720\n",
      "Epoch 758/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5211 - acc: 0.7615 - val_loss: 0.4926 - val_acc: 0.7700\n",
      "Epoch 759/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5197 - acc: 0.7715 - val_loss: 0.4952 - val_acc: 0.7710\n",
      "Epoch 760/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5260 - acc: 0.7660 - val_loss: 0.4919 - val_acc: 0.7720\n",
      "Epoch 761/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5243 - acc: 0.7705 - val_loss: 0.4942 - val_acc: 0.7700\n",
      "Epoch 762/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5240 - acc: 0.7610 - val_loss: 0.4907 - val_acc: 0.7710\n",
      "Epoch 763/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5321 - acc: 0.7575 - val_loss: 0.4936 - val_acc: 0.7730\n",
      "Epoch 764/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5272 - acc: 0.7625 - val_loss: 0.4938 - val_acc: 0.7730\n",
      "Epoch 765/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5263 - acc: 0.7630 - val_loss: 0.4930 - val_acc: 0.7720\n",
      "Epoch 766/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5222 - acc: 0.7620 - val_loss: 0.4936 - val_acc: 0.7720\n",
      "Epoch 767/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5238 - acc: 0.7635 - val_loss: 0.4927 - val_acc: 0.7720\n",
      "Epoch 768/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5290 - acc: 0.7615 - val_loss: 0.4909 - val_acc: 0.7710\n",
      "Epoch 769/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5383 - acc: 0.752 - 0s 30us/step - loss: 0.5268 - acc: 0.7595 - val_loss: 0.4938 - val_acc: 0.7740\n",
      "Epoch 770/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5246 - acc: 0.7700 - val_loss: 0.4923 - val_acc: 0.7720\n",
      "Epoch 771/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5245 - acc: 0.7655 - val_loss: 0.4926 - val_acc: 0.7710\n",
      "Epoch 772/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5185 - acc: 0.7685 - val_loss: 0.4927 - val_acc: 0.7730\n",
      "Epoch 773/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5261 - acc: 0.7585 - val_loss: 0.4913 - val_acc: 0.7740\n",
      "Epoch 774/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5262 - acc: 0.7600 - val_loss: 0.4962 - val_acc: 0.7720\n",
      "Epoch 775/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5222 - acc: 0.7705 - val_loss: 0.4955 - val_acc: 0.7760\n",
      "Epoch 776/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5294 - acc: 0.7665 - val_loss: 0.4937 - val_acc: 0.7740\n",
      "Epoch 777/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5247 - acc: 0.7595 - val_loss: 0.4915 - val_acc: 0.7720\n",
      "Epoch 778/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5235 - acc: 0.7630 - val_loss: 0.4937 - val_acc: 0.7730\n",
      "Epoch 779/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5241 - acc: 0.7590 - val_loss: 0.4929 - val_acc: 0.7750\n",
      "Epoch 780/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5331 - acc: 0.7540 - val_loss: 0.4970 - val_acc: 0.7740\n",
      "Epoch 781/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5257 - acc: 0.7605 - val_loss: 0.4924 - val_acc: 0.7740\n",
      "Epoch 782/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5295 - acc: 0.7680 - val_loss: 0.4925 - val_acc: 0.7720\n",
      "Epoch 783/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5234 - acc: 0.7645 - val_loss: 0.4948 - val_acc: 0.7730\n",
      "Epoch 784/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5375 - acc: 0.7610 - val_loss: 0.4935 - val_acc: 0.7720\n",
      "Epoch 785/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5191 - acc: 0.7710 - val_loss: 0.4927 - val_acc: 0.7730\n",
      "Epoch 786/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5217 - acc: 0.7605 - val_loss: 0.4937 - val_acc: 0.7720\n",
      "Epoch 787/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5230 - acc: 0.7645 - val_loss: 0.4914 - val_acc: 0.7740\n",
      "Epoch 788/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5254 - acc: 0.7620 - val_loss: 0.4904 - val_acc: 0.7720\n",
      "Epoch 789/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5281 - acc: 0.7590 - val_loss: 0.4928 - val_acc: 0.7710\n",
      "Epoch 790/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5241 - acc: 0.7645 - val_loss: 0.4923 - val_acc: 0.7710\n",
      "Epoch 791/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5252 - acc: 0.7685 - val_loss: 0.4948 - val_acc: 0.7690\n",
      "Epoch 792/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5271 - acc: 0.7675 - val_loss: 0.4912 - val_acc: 0.7730\n",
      "Epoch 793/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5220 - acc: 0.7640 - val_loss: 0.4904 - val_acc: 0.7750\n",
      "Epoch 794/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5223 - acc: 0.7635 - val_loss: 0.4925 - val_acc: 0.7720\n",
      "Epoch 795/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5201 - acc: 0.7650 - val_loss: 0.4933 - val_acc: 0.7720\n",
      "Epoch 796/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5250 - acc: 0.7670 - val_loss: 0.4942 - val_acc: 0.7720\n",
      "Epoch 797/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5280 - acc: 0.7625 - val_loss: 0.4921 - val_acc: 0.7710\n",
      "Epoch 798/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5226 - acc: 0.7630 - val_loss: 0.4922 - val_acc: 0.7740\n",
      "Epoch 799/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5288 - acc: 0.7675 - val_loss: 0.4920 - val_acc: 0.7730\n",
      "Epoch 800/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5256 - acc: 0.7575 - val_loss: 0.4936 - val_acc: 0.7730\n",
      "Test loss: 0.4935992879867554\n",
      "Test accuracy: 0.773\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64*2, activation='sigmoid', input_shape=(col-1,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64*2, activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3492397e-01 1.2248373e-02 4.5282772e-01]\n",
      " [8.1894338e-01 3.4179486e-04 1.8071489e-01]\n",
      " [6.5504330e-01 2.6713951e-06 3.4495395e-01]\n",
      " ...\n",
      " [1.4068453e-04 6.9625175e-01 3.0360764e-01]\n",
      " [1.1654807e-11 5.6935307e-02 9.4306475e-01]\n",
      " [2.3892335e-06 7.3740596e-01 2.6259163e-01]]\n",
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(np.shape(classes)) #probability distribution for each of the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "#plotting the training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(acc)+1)\n",
    "plt.plot(epochs,loss,'bo',label='Training loss') #bo for blue dot\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc=history_dict['acc']\n",
    "val_acc=history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label='Training acc')\n",
    "plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
